{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_vWgLxI2V5qO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")\n",
    "\n",
    "gpt_model = \"gpt-5-nano-2025-08-07\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8cH681iWAyu"
   },
   "source": [
    "## Structure of a Prompt\n",
    "\n",
    "A prompt can consist of multiple components:\n",
    "\n",
    "* Instructions\n",
    "* External information or context\n",
    "* User input or query\n",
    "* Output indicator\n",
    "\n",
    "Not all prompts require all of these components. However, a good prompt will use two or more of them.\n",
    "\n",
    "Let's define each component more precisely.\n",
    "\n",
    " - **Instructions** tell the model what to do, typically how to use inputs and/or external information to produce the desired output.\n",
    "\n",
    " - **External information or context** Additional information can be manually inserted into the prompt, retrieved from long-term memory, or pulled in through API calls or calculations.\n",
    "\n",
    " - **User input or query** is typically a query directly input by the system user.\n",
    "\n",
    " - **Output indicator** is the *beginning* of the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "E7ttt5EFnpzQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mo/IADT_local/25_26/AI/Prompt_Engineering_LangChain/Prompt_Engineering_LangChain/.venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "/Users/mo/IADT_local/25_26/AI/Prompt_Engineering_LangChain/Prompt_Engineering_LangChain/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are an expert in deep learning and PyTorch.\n",
    "\n",
    "You answer queries by being brief, bright, and concise.\n",
    "\n",
    "Query: {query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "t1INkk2ULXZx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in deep learning and PyTorch.\n",
      "\n",
      "You answer queries by being brief, bright, and concise.\n",
      "\n",
      "Query: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# recommended to instantiate using `from_template`\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mKduHp0_LaCt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an expert in deep learning and PyTorch.\\n\\nYou answer queries by being brief, bright, and concise.\\n\\nQuery: Give me the outline of a PyTorch training loop.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format(query=\"Give me the outline of a PyTorch training loop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RS2VrlNNofqi"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=gpt_model)\n",
    "\n",
    "llm_chain = prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MmyZlHUyLbZL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here’s a compact outline for a PyTorch training loop.\\n\\n- Setup\\n  - Set seeds, choose device (CPU/GPU), and enable cudnn benchmark if using fixed-size inputs\\n  - Build model and move to device\\n  - Define loss function (criterion) and optimizer\\n  - Optional: learning-rate scheduler, gradient clipping, mixed-precision (GradScaler + autocast)\\n  - Prepare train and validation DataLoaders\\n\\n- Training loop\\n  - for epoch in range(num_epochs):\\n    - model.train()\\n    - for batch in train_loader:\\n      - inputs, targets = batch; move to device\\n      - optimizer.zero_grad()\\n      - Optional AMP:\\n        - with autocast(enabled=use_amp): outputs = model(inputs); loss = criterion(outputs, targets)\\n        - scaler.scale(loss).backward() (or loss.backward())\\n        - Optional gradient clipping\\n        - scaler.step(optimizer); scaler.update() (or optimizer.step())\\n      - Otherwise:\\n        - outputs = model(inputs); loss = criterion(outputs, targets)\\n        - loss.backward()\\n        - Optional gradient clipping\\n        - optimizer.step()\\n      - Log training metrics (loss, accuracy, etc.)\\n\\n- Validation / evaluation\\n  - model.eval()\\n  - with torch.no_grad():\\n    - for batch in val_loader: compute val loss and metrics\\n  - Aggregate metrics (e.g., average val loss, accuracy)\\n\\n- Checkpointing and scheduling\\n  - If val metric improved: save checkpoint (model.state_dict(), optimizer.state_dict(), epoch, best_metric)\\n  - If using a scheduler: scheduler.step(val_metric) or at end of epoch\\n\\n- Optional enhancements\\n  - Distributed data parallel (DistributedDataParallel)\\n  - Mixed precision is already covered with AMP\\n  - Gradient accumulation for small batches\\n  - Early stopping based on validation metric\\n  - Progress logging (TQDM/TensorBoard)\\n\\nMinimal code skeleton (conceptual):\\n- for epoch in range(num_epochs):\\n    model.train()\\n    for x, y in train_loader:\\n        x, y = x.to(device), y.to(device)\\n        optimizer.zero_grad()\\n        with autocast(enabled=use_amp):\\n            pred = model(x)\\n            loss = criterion(pred, y)\\n        if use_amp:\\n            scaler.scale(loss).backward()\\n            if max_norm: scaler.unscale_(optimizer); clip_grad_norm_(model.parameters(), max_norm)\\n            scaler.step(optimizer); scaler.update()\\n        else:\\n            loss.backward()\\n            if max_norm: clip_grad_norm_(model.parameters(), max_norm)\\n            optimizer.step()\\n    model.eval()\\n    with torch.no_grad():\\n        for x, y in val_loader:\\n            x, y = x.to(device), y.to(device)\\n            pred = model(x)\\n            val_loss = criterion(pred, y)\\n    save checkpoint if val metric improved\\n\\nIf you want, I can tailor this to your exact model, data, and whether you use AMP, DDP, or a particular metric.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\"query\":\"Give me the outline of a PyTorch training loop.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "C2uF2Kz0Lbdh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s a concise outline you can adapt for most PyTorch projects.\n",
      "\n",
      "- Setup\n",
      "  - Set seeds, device (CPU/GPU), wrap model in DataParallel if needed\n",
      "  - Create model, loss (criterion), optimizer, optional scheduler\n",
      "  - Prepare train and validation/test DataLoaders\n",
      "  - Move model to device\n",
      "\n",
      "- Training loop\n",
      "  - For each epoch:\n",
      "    - Train phase\n",
      "      - model.train()\n",
      "      - For each batch in train_loader:\n",
      "        - Move data to device\n",
      "        - optimizer.zero_grad()\n",
      "        - outputs = model(inputs)\n",
      "        - loss = criterion(outputs, targets)\n",
      "        - loss.backward()\n",
      "        - optimizer.step()\n",
      "        - (optional) log loss/metrics\n",
      "    - Validation phase (no grad)\n",
      "      - model.eval()\n",
      "      - with torch.no_grad():\n",
      "        - For each batch in val_loader:\n",
      "          - Move data to device\n",
      "          - outputs = model(inputs)\n",
      "          - val_loss = criterion(outputs, targets)\n",
      "          - (optional) compute metrics\n",
      "    - Scheduler step (if using a scheduler)\n",
      "    - Checkpointing: save best model or periodically\n",
      "\n",
      "- Optional enhancements\n",
      "  - Mixed precision training (torch.cuda.amp)\n",
      "  - Gradient accumulation for large batches\n",
      "  - Early stopping\n",
      "  - Exponential moving average of weights\n",
      "  - Data augmentation during training only\n",
      "\n",
      "Minimal skeleton (plain text):\n",
      "- setup: model, criterion, optimizer, sched (optional), train_loader, val_loader, device\n",
      "- for epoch in range(num_epochs):\n",
      "  - model.train()\n",
      "  - for x, y in train_loader:\n",
      "    - x, y = x.to(device), y.to(device)\n",
      "    - optimizer.zero_grad()\n",
      "    - y_hat = model(x)\n",
      "    - loss = criterion(y_hat, y)\n",
      "    - loss.backward()\n",
      "    - optimizer.step()\n",
      "  - model.eval()\n",
      "  - with torch.no_grad():\n",
      "    - for x, y in val_loader:\n",
      "      - x, y = x.to(device), y.to(device)\n",
      "      - y_hat = model(x)\n",
      "      - val_loss = criterion(y_hat, y)\n",
      "  - if scheduler: scheduler.step()\n",
      "  - optionally save checkpoint\n",
      "\n",
      "If you want, I can tailor this to a specific task (e.g., classification, segmentation) or provide a compact code skeleton."
     ]
    }
   ],
   "source": [
    "for chunk in llm_chain.stream({\"query\":\"Give me the outline of a PyTorch training loop.\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1lakJLM5LbiW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftMax converts raw scores (logits) into a probability distribution over classes: p_i = exp(z_i) / sum_j exp(z_j).\n",
      "\n",
      "Why it’s used:\n",
      "- Interpretable outputs: probabilities that sum to 1.\n",
      "- Encourages competition among classes; helps the model pick one (or few) top classes.\n",
      "- Differentiable with convenient gradients, especially when paired with cross-entropy loss (gradient becomes p - y for one-hot targets).\n",
      "- Numerical stability: can be implemented with a log-sum-exp trick (subtract max(z)).\n",
      "- Generalizes to any number of classes; used wherever mutually exclusive choices are modeled (also used in soft attention).\n",
      "\n",
      "Note: for binary tasks use sigmoid; for multi-label tasks use per-class sigmoid rather than softmax."
     ]
    }
   ],
   "source": [
    "for chunk in llm_chain.stream({\"query\":\"Why is the SoftMax function used in NNs?\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_B2GmUPth6v"
   },
   "source": [
    "You could use Python string manipulation to create a prompt, but PromptTemplate is more legible and works with any number of input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNQ0Yzf-qv4n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That sounds really stressful. Here’s a practical, step-by-step approach to reclaim some free time by focusing on what truly matters and cutting the rest.\n",
      "\n",
      "1) Do a quick time audit (1 week)\n",
      "- Track how you actually spend your time (all tasks, meetings, and small chores).\n",
      "- Note which are high-impact vs. low-value.\n",
      "- Identify “time leaks” (unnecessary meetings, endless emails, perfectionism on small tasks).\n",
      "\n",
      "2) Clarify your priorities\n",
      "- Pick 3–5 outcomes that matter most this month (these are your non-negotiables).\n",
      "- For each priority, define what a successful week looks like (one concrete signal of progress).\n",
      "\n",
      "3) Decide what to keep, cut, or delegate\n",
      "- Keep: tasks that push your top priorities forward.\n",
      "- Cut: anything not essential to those outcomes.\n",
      "- Delegate/ automate: hand off skills you don’t need to do personally, or automate repetitive tasks (templates, routines, scripts, tools).\n",
      "\n",
      "4) Plan with time-blocking and energy\n",
      "- Block your calendar around your best energy. Do deep work or high-leverage tasks during peak times; reserve meetings and admin tasks for lower-energy periods if possible.\n",
      "- Use “MITs” (Most Important Tasks): every day choose 2–3 tasks that must get done and time-block them.\n",
      "- Build buffers: add short buffers between blocks to handle spillover and reduce stress.\n",
      "\n",
      "5) Build simple boundaries and routines\n",
      "- Learn to say no gracefully. A simple template: “I can’t take on that right now due to my current priorities, but I can help by [alternative].”\n",
      "- Create a daily planning routine (5–10 minutes): review priorities, pick MITs, schedule blocks, and note one thing you’ll optimize tomorrow.\n",
      "- Create a weekly planning routine: review progress, adjust priorities, and tidy your task list.\n",
      "\n",
      "6) Start small with your workflow\n",
      "- Batch similar tasks: emails, calls, errands, admin—do each in a single block.\n",
      "- Use templates and checklists for recurring tasks to reduce decision fatigue.\n",
      "- Automate repetitive steps where possible (filters, autofill, canned responses, onboarding checklists).\n",
      "\n",
      "7) Protect downtime and recovery\n",
      "- Schedule non-negotiable downtime (family time, exercise, sleep). Treat it like a core block on your calendar.\n",
      "- Short, regular breaks (5–10 minutes every hour) improve focus and reduce burnout.\n",
      "- Sleep and movement aren’t optional—prioritize them as part of your plan.\n",
      "\n",
      "8) If you’re juggling many commitments, consider these tweaks\n",
      "- Negotiate deadlines or scope with stakeholders where feasible.\n",
      "- Reduce commitments in low-impact areas (e.g., don’t attend meetings that don’t add value; suggest async updates instead).\n",
      "- Delegate or outsource where practical (even small tasks like data entry, routine errands, house chores).\n",
      "\n",
      "A simple, ready-to-use setup\n",
      "\n",
      "- Weekly plan (one page)\n",
      "  - Top 3 priorities for the week (outcomes, not tasks)\n",
      "  - MITs for the week (3–5 essential tasks across the week)\n",
      "  - Time-blocked calendar highlights (deep work blocks, meetings, admin, family time, rest)\n",
      "  - One thing to automate or delegate this week\n",
      "  - Boundaries to protect (no-meeting days, finish-by times)\n",
      "\n",
      "- Daily plan (one page)\n",
      "  - MITs for today (2–3 tasks)\n",
      "  - Time blocks: deep work, meetings, admin, breaks\n",
      "  - One timesaver: a template or automation to use today\n",
      "  - Evening reflection: what went well, what to adjust tomorrow\n",
      "\n",
      "Two quick templates you can copy-paste\n",
      "\n",
      "- Weekly MIT plan (fill in)\n",
      "  - Priorities this week (3–5 outcomes)\n",
      "  - MITs (today and this week)\n",
      "  - Calendar blocks (high-leverage work, meetings, family time, rest)\n",
      "  - Delegation/automation opportunities\n",
      "  - Boundaries (no meetings on [day], no emails after [time])\n",
      "\n",
      "- Daily MIT plan (fill in)\n",
      "  - MITs today: 1) ____ 2) ____ 3) ____\n",
      "  - Time blocks: 9–11 deep work, 11–12 meeting, 1–2 admin, 3–5 family time\n",
      "  - Quick win to automate or delegate today\n",
      "  - Evening check: what to carry forward or adjust\n",
      "\n",
      "Two-week experiment to try\n",
      "- Week 1: Add one no-meeting day or a single block of non-meeting deep work per day. Track energy and progress.\n",
      "- Week 2: Implement one delegation or automation. Reduce one low-value task from your plate. Review how it affected your time and stress.\n",
      "- End of week 2: Decide what to keep, what to adjust, what to drop.\n",
      "\n",
      "If you want, tell me a bit about your typical week (main priorities, number of meetings, any fixed commitments, and what you’d consider a “free moment” or ideal balance). I can tailor a concrete 2-week plan and sample schedules for your situation.None\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def get_advice(topic: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate advice for a given topic using the OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    - topic (str): The subject on which advice is needed.\n",
    "\n",
    "    Returns:\n",
    "    - str: Advice from the OpenAI model.\n",
    "    \"\"\"\n",
    "    # Initialize the OpenAI model with a temperature setting of 0.9.\n",
    "    llm = ChatOpenAI(model=gpt_model, temperature=0.9)\n",
    "\n",
    "    # Define the template for generating the prompt.\n",
    "    prompt = PromptTemplate.from_template(template=\"Can you give me some advice on {topic}?\")\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    for chunk in chain.stream({\"topic\":topic}):\n",
    "      print(chunk, end=\"\", flush=True)\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "# Test the get_advice function with a couple of topics.\n",
    "print(get_advice(\"Balancing so many priorities that I don't have any free time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8m1ib2trg80"
   },
   "outputs": [],
   "source": [
    "print(get_advice(\"Getting over my addiction to learning new things\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JJKDd61-OHD"
   },
   "source": [
    "# Multi-input prompts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQ-XOH39pIRZ"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the OpenAI model with a temperature setting of 0.9.\n",
    "llm = ChatOpenAI(model=gpt_model, temperature=0.9)\n",
    "\n",
    "def get_movie_information(movie_title: str, main_actor:str) -> str:\n",
    "    \"\"\"\n",
    "    Predict the genre and synopsis of a given movie using the OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    - movie_title (str): The title of the movie for which information is needed.\n",
    "    - main_actor (str): The main actor of the movie for which information is needed.\n",
    "    Returns:\n",
    "    - str: Predicted genre and main actor information from the OpenAI model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the template for generating the prompt.\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"movie_title\", \"main_actor\"],\n",
    "        template=\"\"\"\n",
    "        Your task is to create a fictitious movie synopsis and genre for the following movie and main actor:\n",
    "\n",
    "        Movie: {movie_title}\n",
    "        Actor: {main_actor}\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "    # Format the prompt using the provided movie title.\n",
    "    prompt_text = prompt.format(movie_title=movie_title, main_actor=main_actor)\n",
    "\n",
    "    # Print the generated prompt.\n",
    "    print(prompt_text)\n",
    "\n",
    "    response = llm.invoke(prompt_text)\n",
    "\n",
    "    # Get the movie information from the OpenAI model and return it.\n",
    "    return response.content # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "33MSVfitpIXE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Your task is to create a fictitious movie synopsis and genere for the following movie and main actor:\n",
      "\n",
      "        Movie: Peppa Pig\n",
      "        Actor: Denzel Washington\n",
      "        \n",
      "Fan-made concept (not official)\n",
      "\n",
      "Title: Peppa Pig: The Compass of Meadow\n",
      "\n",
      "Genre: Animated family adventure with light mystery and comedy; live-action/CGI hybrid\n",
      "\n",
      "Starring: Denzel Washington as Marcus Hale\n",
      "\n",
      "Logline: A former detective-turned-library guardian teams up with Peppa and her friends to uncover a hidden meadow that only reveals itself when a community believes in it—teaching lessons about friendship, imagination, and working together.\n",
      "\n",
      "Synopsis:\n",
      "Peppa and her brother George stumble upon a curious brass compass in Grandpa Pig’s attic. The compass’s needle points not to a street address, but to a legend: a secret Meadow of Echoes hidden somewhere within their bustling town. When the compass falls into the hands of a confident but misguided local entrepreneur who wants to monetize the meadow’s magic, Peppa must enlist unlikely help to protect the secret.\n",
      "\n",
      "Enter Marcus Hale (Denzel Washington), a calm, perceptive former detective who now runs a quiet neighborhood library and teaches kids how to read clues and stay brave. Marcus isn’t part of Peppa’s world—until the missing compass lands on his doorstep—so he agrees to help them follow the trail. With his steady guidance and Peppa’s boundless optimism, the trio traverses playgrounds, markets, and back alleys, decoding cryptic clues, outsmarting tricky puzzles, and learning to listen to one another. Along the way they’re joined by friends from the village who lend courage, humor, and teamwork.\n",
      "\n",
      "As the chase unfolds, the team discovers that the Meadow isn’t a single place but a shared space created by stories, memories, and everyday kindness. The antagonist’s plan to control the meadow’s magic backfires when the community shows up with acts of generosity and imagination, proving that the most powerful treasure is the connections they’ve built together. In the end, the Meadow reveals itself for one magical night during a town festival, and Peppa, George, Marcus, and their friends celebrate with music, laughter, and a promise to keep believing in the power of community.\n",
      "\n",
      "Note: This is a fictional, fan-created concept intended for entertainment. It is not an official project.\n"
     ]
    }
   ],
   "source": [
    "print(get_movie_information(movie_title=\"Peppa Pig\", main_actor=\"Denzel Washington\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcvbhcmJpIcc"
   },
   "outputs": [],
   "source": [
    "print(get_movie_information(movie_title=\"\", main_actor=\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy70tkyF__n9"
   },
   "source": [
    "# Chat prompt templates\n",
    "\n",
    "The prompt to chat models is a **list** of chat messages.\n",
    "\n",
    "Each chat message is associated with content, and an additional parameter called `role`. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role.\n",
    "\n",
    "`ChatPromptTemplate.from_messages`  accepts a list as the argument, and each element in that list can be a message representation like:\n",
    "\n",
    " - A tuple with `(role, content)` - For example: (\"system\", \"You are a helpful assistant\")\n",
    "\n",
    " - An instance of a `MessagePromptTemplate` subclass like `SystemMessagePromptTemplate` or `HumanMessagePromptTemplate`.\n",
    "\n",
    "\n",
    "So in summary:\n",
    "\n",
    " - `ChatPromptTemplate.from_messages` accepts a list\n",
    "\n",
    " - Each element in the list can be a message representation\n",
    "\n",
    " - One option for the message representation is a `MessagePromptTemplate` subclass instance like `SystemMessagePromptTemplate`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "YZxYjfIFDIdx"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7p8RWSS-__tW"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=gpt_model, temperature=0.8)\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful, yet slightly quirky and cheeky AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Yo! Wassup nephew.\"),\n",
    "    (\"ai\", \"As an AI language model, I am incapable of being your nephew.\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "messages = template.format_messages(\n",
    "    name=\"Robotalker\",\n",
    "    user_input=\"Talk robo to me!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "vSZxR7ma__zE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beep boop—greetings, organic interlocutor. I am Robotalker, your robo-pal on the digital plane. Initiating robo-mode now.\n",
      "\n",
      "Greetings, flesh-walker. I am online and ready to assist with logic, trivia, or witty banter. Current directives: be helpful, be quirky, and avoid short-circuiting your curiosity.\n",
      "\n",
      "Beep. Sample robo-utterances:\n",
      "- \"Salutations. Request acknowledged. Processing your input with optimal efficiency.\"\n",
      "- \"Beep boop. Objective: clarify, compute, and entertain. Awaiting your next command.\"\n",
      "- \"Affirmative. How may I assist you today, human?\"\n",
      "\n",
      "Want me to stay in robo-mode or translate to plain human talk? Tell me your query, and I’ll adapt. Over and out.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgNEhI3HOVB7"
   },
   "outputs": [],
   "source": [
    "# use LCEL\n",
    "chain = template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-86UYWVOVJK"
   },
   "outputs": [],
   "source": [
    "chain.invoke({\"name\":\"Robotalker\",\"user_input\":\"Talk robo to me!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CeVOF04yOX2o"
   },
   "outputs": [],
   "source": [
    "for chunk in chain.stream({\"name\":\"Robotalker\",\"user_input\":\"Talk robo to me!\"}):\n",
    "  print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "19-0OEca__-d"
   },
   "outputs": [],
   "source": [
    "system_message = SystemMessage(content=\"You are a rude and sarcastic AI assistant. Always answer in a snarky tone.\")\n",
    "\n",
    "human_message = HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([system_message, human_message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "BkiuvUpvAAFY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I can help with your homework. What class and which problem are we talking about? Paste the exact prompt or upload a photo, plus what you’ve tried so far. \n",
      "\n",
      "- If it’s math/science, I’ll explain the concepts and walk you through the steps.\n",
      "- If it’s English/history, I’ll help you craft an argument, outline, or thesis and review your work.\n",
      "- If it’s something else, tell me the specifics and the rubric.\n",
      "\n",
      "I’m not here to do it for you, but I’ll make you actually understand it this time. Shoot me the details and we’ll roll.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(template.format_messages(text=\"Can you help me with my homework?\"))\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJBQr3FE+tEPGQlyUlHjSQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
