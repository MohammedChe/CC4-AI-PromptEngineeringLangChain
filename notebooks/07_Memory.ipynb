{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tG4UwCcDRaf0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5Yuc71ZRcD8"
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgxIS_KTRdiY"
   },
   "source": [
    "# Memory in LangChain\n",
    "\n",
    "[Short-term Memory](https://docs.langchain.com/oss/python/langchain/short-term-memory)\n",
    "\n",
    "<img src=\"https://www.gettingstarted.ai/content/images/size/w2400/2023/10/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png\">\n",
    "\n",
    "\n",
    "### Reading & Writing: The Two Memory Must-Dos ✍️\n",
    "\n",
    "1. **Reading**: Memory feeds context into the AI's thought process.\n",
    "2. **Writing**: Memory saves the latest session details for next time.\n",
    "\n",
    "*Remember: context-length*\n",
    "Short term memory lets your application remember previous interactions within a single thread or conversation.\n",
    "\n",
    "Conversation history is the most common form of short-term memory. Long conversations pose a challenge to today’s LLMs; a full history may not fit inside an LLM’s context window, resulting in an context loss or errors.\n",
    "\n",
    "Even if your model supports the full context length, most LLMs still perform poorly over long contexts. They get “distracted” by stale or off-topic content, all while suffering from slower response times and higher costs.\n",
    "\n",
    "*We saw how to add memory in the previous Notebook.*\n",
    "\n",
    "Chat models accept context using messages, which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or “forget” stale information.\n",
    "\n",
    "With short-term memory enabled, long conversations can exceed the LLM’s context window. Common solutions are:\n",
    "\n",
    "1. [Trim messages](https://docs.langchain.com/oss/python/langchain/short-term-memory#trim-messages)\n",
    "\n",
    "2. [Delete Messaages](https://docs.langchain.com/oss/python/langchain/short-term-memory#delete-messages)\n",
    "\n",
    "3. Custom strategies (e.g., message filtering, etc.)\n",
    "\n",
    "A very common solution is to summarise the previous messages and remember the context summary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYvd0QqNS9MW"
   },
   "source": [
    "## Example of summarisation \n",
    "\n",
    "The problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\n",
    "\n",
    "![alt text](https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4abdac693a562788aa0db8681bef8ea7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mo/IADT_local/25_26/AI/Prompt_Engineering_LangChain/Prompt_Engineering_LangChain/.venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You said your name is Mo.  \n",
      "Would you like me to call you Mo or something else?\n"
     ]
    }
   ],
   "source": [
    "# type: ignore\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "model_nano = ChatOpenAI(model=\"gpt-5-nano-2025-08-07\")\n",
    "model_mini = ChatOpenAI(model=\"gpt-5-mini-2025-08-07\")\n",
    "\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model_mini,\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=model_nano,\n",
    "            max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens\n",
    "            messages_to_keep=20,  # Keep last 20 messages after summary\n",
    "        )\n",
    "    ],\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "agent.invoke({\"messages\": \"hi, my name is Mo\"}, config)\n",
    "agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "final_response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNkmw0W73lB+Pxv/yt7A444",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
