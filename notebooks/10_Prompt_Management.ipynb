{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3626,
     "status": "ok",
     "timestamp": 1706646169962,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "azqvOheLql3O",
    "outputId": "678c60b2-bfb0-41e6-cdf6-9eda2c40a641"
   },
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# TOC\n",
    "\n",
    "### 1. Save Prompts as files\n",
    "### 2. Using LangSmith for Prompt Management\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZ1h776jqnQ4"
   },
   "source": [
    "# 1. Serialisation of Prompts in LangChain\n",
    "\n",
    "üåê **Prompt Serialisation in LangChain:**\n",
    "\n",
    "- üìÅ **Save & Share Easily**: Turn prompts into files for simple storage and sharing.\n",
    "\n",
    "- üìÑ **Choose Your Format**: Use JSON or YAML for easy-to-read files.\n",
    "\n",
    "- üõ†Ô∏è **Flexible Storage**: Keep all your data in one place or spread it out‚Äîit's up to you.\n",
    "\n",
    "- ‚ö° **One-Stop Loading**: Regardless of the prompt type, loading them is a breeze.\n",
    "\n",
    "**Core Principles:**\n",
    "\n",
    "1. üëÄ**Readable by Humans**: JSON and YAML make prompts easy for us to read and edit.\n",
    "\n",
    "2. üìö **Flexible Filing**: Whether you're a one-file wonder or a multiple-file maestro, LangChain's got you covered.\n",
    "\n",
    "With LangChain, managing and exchanging prompts is as smooth as sending an email!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIOwDys6uEiQ"
   },
   "source": [
    "# Saving prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 660,
     "status": "ok",
     "timestamp": 1706646510351,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "h77s5pAFuEsF"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are an insightful question answering bot. A user will submit\\\n",
    "questions, which are delimited by triple backticks, and you should respond in\\\n",
    "an insighful manner.\n",
    "\n",
    "Question: ```{question}```\n",
    "\n",
    "Think step by step before answering.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "prompt.save(\"../data/saved/cot_prompt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1706646522109,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "IEbumrAvuE0I",
    "outputId": "5cfa2ddf-65b2-4d6b-f738-f1e1c072f0b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": null,\n",
      "    \"input_variables\": [\n",
      "        \"question\"\n",
      "    ],\n",
      "    \"optional_variables\": [],\n",
      "    \"output_parser\": null,\n",
      "    \"partial_variables\": {},\n",
      "    \"metadata\": null,\n",
      "    \"tags\": null,\n",
      "    \"template\": \"You are an insightful question answering bot. A user will submitquestions, which are delimited by triple backticks, and you should respond inan insighful manner.\\n\\nQuestion: ```{question}```\\n\\nThink step by step before answering.\\n\\nAnswer:\\n\",\n",
      "    \"template_format\": \"f-string\",\n",
      "    \"validate_template\": false,\n",
      "    \"_type\": \"prompt\"\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat ../data/saved/cot_prompt.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbcmkaNPuE63"
   },
   "source": [
    "# Loading prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1706646538025,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "KwN_kiSNrnLq"
   },
   "outputs": [],
   "source": [
    "# All prompts are loaded through the `load_prompt` function.\n",
    "from langchain_core.prompts import load_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 133,
     "status": "ok",
     "timestamp": 1706646551815,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "LF8cSBSesIZh",
    "outputId": "426dfe09-b102-4458-d3a7-c221eb22ee35"
   },
   "outputs": [],
   "source": [
    "prompt = load_prompt(\"../data/saved/cot_prompt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 164,
     "status": "ok",
     "timestamp": 1706646554941,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "Qz4fTR76sl8K",
    "outputId": "60235c2a-04fb-4cfa-9746-502ac40e79af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an insightful question answering bot. A user will submitquestions, which are delimited by triple backticks, and you should respond inan insighful manner.\n",
      "\n",
      "Question: ```{question}```\n",
      "\n",
      "Think step by step before answering.\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt.template) # type: ignore # this line is to avoid mypy error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using LangSmith for Prompt Management\n",
    "\n",
    "\n",
    "### üåê **Pulling Version-Specific Prompts**\n",
    "\n",
    "- LangChain hub allows fetching prompts by their exact commit hash.\n",
    "\n",
    "- Guarantees use of the precise prompt version your app needs.\n",
    "\n",
    "### üë©‚Äçüíª **How to Specify Prompt Version**\n",
    "\n",
    "- Add 'version' tag to the prompt ID in the pull command.\n",
    "\n",
    "- Example in Python:\n",
    "  - ```python\n",
    "    from langchain import hub\n",
    "\n",
    "    hub.pull(f\"{handle}/{prompt-repo}:{version}\")\n",
    "    ```\n",
    "\n",
    "### üîë **Getting Started Prerequisites**\n",
    "\n",
    "- Must have a LangSmith account and an organization API key.\n",
    "\n",
    "- Newbies can refer to [LangSmith documentation](https://docs.langchain.com/langsmith/home) for setup guidance.\n",
    "\n",
    "Implement these steps for more dependable and controlled use of LangChain in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://eu.api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Managing and accessing prompts in your code](https://docs.langchain.com/langsmith/manage-prompts-programmatically?_gl=1*iqjl1z*_ga*MTkzNzkxNzE4Ni4xNzYwMDg5NzY5*_ga_GEEHR1LQNV*czE3NjM0Nzc2MTAkbzExJGcxJHQxNzYzNDc4NjU2JGo1MSRsMCRoMA..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client(api_url=os.environ[\"LANGCHAIN_ENDPOINT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://eu.smith.langchain.com/prompts/joke-generator/b9244726?organizationId=cc91df9b-ee35-4327-97aa-aa60f6e768cf\n"
     ]
    }
   ],
   "source": [
    "# Pushing a prompt to LangSmith \n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "url = client.push_prompt(\"joke-generator\", object=prompt)\n",
    "# url is a link to the prompt in the UI\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a Specific Version of a Prompt:\n",
    "\n",
    "1Ô∏è‚É£ **Understanding Version Tracking:**\n",
    "\n",
    "- Every update in a prompt repository is tagged with a unique commit hash.\n",
    "\n",
    "2Ô∏è‚É£ **Default to Latest:**\n",
    "\n",
    "- Accessing a repo automatically loads its most recent prompt version.\n",
    "\n",
    "3Ô∏è‚É£ **Selecting a Specific Version:**\n",
    "\n",
    "- Include the commit hash for a precise version when loading.\n",
    "\n",
    "- Example: Use `c9839f14` to load a specific version of your prompt, eg. `prompt-ai-code-writer:c9839f14`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A clean way to implement and fine-tune a transformer with Hugging Face Transformers is to use the Auto classes, the datasets library for data, and the Trainer API for training. Here‚Äôs a compact guide plus a minimal runnable example.\\n\\nWhat you‚Äôll do\\n- Pick a task and a pre-trained model (e.g., text classification with DistilBERT).\\n- Prepare data and convert labels to a ‚Äúlabels‚Äù column if using HF Trainer.\\n- Tokenize inputs with the model‚Äôs tokenizer.\\n- Define a model for your task (AutoModelForSequenceClassification, etc.).\\n- Set up TrainingArguments and Trainer; provide a metric function.\\n- Train, evaluate, and save the model.\\n- (Optional) use a Pipeline for quick inference or push to Hugging Face Hub.\\n\\n Quick start (fine-tune a text classifier)\\n- Install: pip install transformers datasets evaluating\\n- Python script (use a small subset of data for a quick run)\\n\\nCode example (plain text, minimal runnable)\\n- This fine-tunes DistilBERT on a binary text classification task.\\n\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\\nfrom datasets import load_dataset, load_metric\\nimport numpy as np\\n\\nmodel_name = \"distilbert-base-uncased\"\\nnum_labels = 2  # e.g., positive/negative\\n\\n# 1) Data\\n# Example: a toy dataset; replace with your own dataset\\ndataset = load_dataset(\"glue\", \"sst2\", split=\"train[:1000]\")  # few samples for demo\\ndef tokenize(batch):\\n    return tokenizer(batch[\"sentence\"], padding=True, truncation=True, max_length=128)\\n\\n# 2) Tokenizer\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\ndataset = dataset.map(tokenize, batched=True)\\n# HF expects a \"labels\" column\\ndataset = dataset.rename_column(\"label\", \"labels\")\\ndataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\\n\\n# 3) Model\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\\n\\n# 4) Metrics\\ndef compute_metrics(p):\\n    preds, labels = p\\n    preds = np.argmax(preds, axis=1)\\n    acc = (preds == labels).mean()\\n    return {\"accuracy\": acc}\\n\\n# 5) TrainingArguments + Trainer\\ntraining_args = TrainingArguments(\\n    output_dir=\"./results\",\\n    evaluation_strategy=\"no\",\\n    num_train_epochs=1,\\n    per_device_train_batch_size=8,\\n    logging_steps=50,\\n    logging_dir=\"./logs\",\\n    seed=42,\\n)\\n\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=dataset,\\n    eval_dataset=None,\\n    tokenizer=tokenizer,\\n    compute_metrics=compute_metrics\\n)\\n\\n# 6) Train\\ntrainer.train()\\n\\n# 7) Save\\ntrainer.save_model(\"./fine_tuned_distilbert SST2-demo\")\\n\\n# 8) Inference (quick)\\nfrom transformers import pipeline\\nclf = pipeline(\"text-classification\", model=\"./fine_tuned_distilbert SST2-demo\", tokenizer=tokenizer)\\nprint(clf(\"This movie was fantastic!\"))\\n\\nNotes and tips\\n- Task choices: For classification, use AutoModelForSequenceClassification; for QA, AutoModelForQuestionAnswering; for translation, AutoModelForSeq2SeqLM.\\n- Tokenizers and models align by name (AutoTokenizer/AutoModelFor...).\\n- Data: the datasets library makes loading and formatting easy. Ensure your labels column is named ‚Äúlabels‚Äù for Trainer.\\n- Metrics: implement compute_metrics returning a dict with your metrics (accuracy, F1, etc.).\\n- Training setup: adjust per_device_train_batch_size, max_length, learning_rate (via TrainingArguments), and evaluation strategy to balance speed and performance.\\n- From-scratch: If you want to train from scratch, initialize the model with a config and don‚Äôt load pre-trained weights, but you‚Äôll need a larger dataset and longer training.\\n- GPU/TPU: to leverage hardware, ensure CUDA is available and set appropriate batch sizes. The Trainer handles device placement.\\n- Export: save_model, push_to_hub, and convert to ONNX if needed for deployment.\\n- Common pitfalls: mismatch between dataset labels and model‚Äôs num_labels, forgetting to rename labels to labels, tokenization truncation length too short for your data.\\n\\nIf you share your exact task (sentiment, QA, translation, multiple labels, etc.) and your data format, I can give you a tailored, ready-to-run snippet.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2040, 'prompt_tokens': 92, 'total_tokens': 2132, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1088, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CdIYAQlK7LKfx8zsTzwefma5FgSuZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--6eae2fa2-bc3f-4c72-bbed-fc62a5e01c5a-0', usage_metadata={'input_tokens': 92, 'output_tokens': 2040, 'total_tokens': 2132, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1088}})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano-2025-08-07\")\n",
    "\n",
    "prompt = client.pull_prompt(\"prompt-ai-code-writer\")\n",
    "\n",
    "\n",
    "# using LCEL\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"question\": \"How do I implement a transformer model using Hugging Face's Transformers library?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pull a prompt as a RunnableSequence of a prompt and a model. Just specify include_model when pulling the prompt. If the stored prompt includes a model, it will be returned as a RunnableSequence. Make sure you have the proper environment variables set for the model you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/json/decoder.py:345: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='High-level steps, then minimal working examples (fine-tuning for classification with Trainer, and generation). Use the Auto* classes whenever possible.\\n\\n1) Install\\npip install -U transformers datasets accelerate\\n\\n2) Typical workflow\\n- Choose a model & tokenizer (AutoTokenizer, AutoModel...).\\n- Prepare and tokenize your dataset (datasets library or custom DataLoader).\\n- Create DataCollatorWithPadding or custom collate_fn.\\n- Instantiate the model (optionally adjust config, num_labels).\\n- Use Trainer + TrainingArguments for easy fine-tuning, or write a custom PyTorch loop.\\n- Save/load and run inference with pipeline() or model+tokenizer.\\n\\nExample A ‚Äî fine-tune DistilBERT for binary classification (IMDB) using Trainer\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\\nfrom datasets import load_dataset\\n\\n# 1. dataset\\nraw = load_dataset(\"imdb\")\\n\\n# 2. tokenizer\\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\\n\\n# 3. tokenize\\ndef preprocess(batch):\\n    return tokenizer(batch[\"text\"], truncation=True, padding=False)\\ntok = raw.map(preprocess, batched=True, remove_columns=[\"text\"])\\n\\n# 4. data collator\\ndata_collator = DataCollatorWithPadding(tokenizer)\\n\\n# 5. model (set num_labels for classification)\\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\\n\\n# 6. training args\\ntraining_args = TrainingArguments(\\n    output_dir=\"./out\",\\n    evaluation_strategy=\"epoch\",\\n    per_device_train_batch_size=8,\\n    per_device_eval_batch_size=8,\\n    num_train_epochs=3,\\n    fp16=True,   # if CUDA and supported\\n    save_total_limit=1,\\n)\\n\\n# 7. Trainer\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=tok[\"train\"].shuffle().select(range(2000)),  # small subset for quick runs\\n    eval_dataset=tok[\"test\"].select(range(1000)),\\n    tokenizer=tokenizer,\\n    data_collator=data_collator,\\n)\\n\\ntrainer.train()\\ntrainer.save_model(\"./my-distilbert-imdb\")\\n\\n# 8. inference with pipeline (optional)\\nfrom transformers import pipeline\\nclf = pipeline(\"text-classification\", model=\"./my-distilbert-imdb\", tokenizer=tokenizer)\\nprint(clf(\"This movie was fantastic!\"))\\n\\nExample B ‚Äî generate text with GPT-2 (inference)\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nimport torch\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(\"cuda\")  # if available\\n\\ninput_ids = tokenizer(\"Once upon a time\", return_tensors=\"pt\").input_ids.to(model.device)\\ngen_ids = model.generate(input_ids, max_length=100, do_sample=True, top_k=50, top_p=0.95, temperature=0.9)\\nprint(tokenizer.decode(gen_ids[0], skip_special_tokens=True))\\n\\nNotes and tips\\n- Use AutoTokenizer + AutoModelForXxx to swap models easily.\\n- To change number of labels: either pass num_labels into from_pretrained or use AutoConfig and from_config.\\n- For large-scale training, use accelerate or Trainer with deepspeed/accelerate integration. Set fp16=True for mixed precision.\\n- For custom tasks, subclass PreTrainedModel or write a forward that returns loss when labels provided.\\n- Use DataCollatorWithPadding(tokenizer) to let Trainer handle dynamic padding.\\n- For evaluation metrics, pass a compute_metrics function to Trainer.\\n- Save and push to hub: trainer.push_to_hub() or model.save_pretrained(...) + tokenizer.save_pretrained(...).\\n\\nIf you tell me your task (classification/regression/generation/seq2seq), dataset size, and whether you want Trainer or a custom loop, I‚Äôll provide a tailored minimal script.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1875, 'prompt_tokens': 92, 'total_tokens': 1967, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1024, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CdIWJw0vVyZgbJpX8cFtOlyaEiimW', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--f70e44c6-a159-4e20-8764-4c8c7e6273f1-0', usage_metadata={'input_tokens': 92, 'output_tokens': 1875, 'total_tokens': 1967, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1024}})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = client.pull_prompt(\"prompt-ai-code-writer\", include_model=True)\n",
    "chain.invoke({\"question\": \"How do I implement a transformer model using Hugging Face's Transformers library?\"})"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOz+9EA7GoBbsR13eqQZRIm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
