{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6611,
     "status": "ok",
     "timestamp": 1706679000028,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "ROv0ygONORNx",
    "outputId": "dd015dc1-952d-42d5-8783-9770243f0327"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your HF API key: \")\n",
    "\n",
    "open_ai_model = \"gpt-5-nano-2025-08-07\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEAWUvegOw27"
   },
   "source": [
    "#ðŸ”— **Origins of CoT Prompting**\n",
    "\n",
    "- CoT was introduced by Wei et al. in their 2022 paper, [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://browse.arxiv.org/pdf/2201.11903.pdf).\n",
    "\n",
    "- It's a response to the need for better reasoning in large language models.\n",
    "\n",
    "ðŸ’¡ **What is CoT Prompting?**\n",
    "- CoT stands for Chain of Thought Prompting.\n",
    "\n",
    "- It's about guiding language models through a step-by-step reasoning process.\n",
    "\n",
    "- The model shows its reasoning, forming a \"chain\" of thoughts, rather than just giving an answer.\n",
    "\n",
    "- Mimics human-like thought processes for complex tasks.\n",
    "\n",
    "\n",
    "### ðŸŽ“ **Standard vs. CoT Prompting Example**\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://deepgram.com/_next/image?url=https%3A%2F%2Fwww.datocms-assets.com%2F96965%2F1696369825-cot-example.png&w=3840&q=75\" alt=\"Image Description\" style=\"width:100%\">\n",
    "  <figcaption>Examples of âŸ¨input, chain of thought, outputâŸ© triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted.</figcaption>\n",
    "  <a href=\"https://deepgram.com/_next/image?url=https%3A%2F%2Fwww.datocms-assets.com%2F96965%2F1696369825-cot-example.png&w=3840&q=75\">Image Source</a>\n",
    "</figure>\n",
    "\n",
    "- Standard: Direct answer with no reasoning (e.g., \"11\").\n",
    "\n",
    "- CoT: Detailed reasoning steps (e.g., \"Roger started with 5, added 6 from 2 cans, so 5 + 6 = 11\").\n",
    "\n",
    "### ðŸš€ **Usage of CoT Prompting**\n",
    "\n",
    "1. **Enhanced Reasoning**: Breaks down complex problems into simpler steps.\n",
    "\n",
    "2. **Combination with Few-Shot Prompting**: Uses examples to guide model responses, great for complex tasks.\n",
    "\n",
    "3. **Interpretability**: Offers a clear view of the model's thought process.\n",
    "\n",
    "4. **Applications**: Useful in arithmetic, commonsense reasoning, and symbolic tasks.\n",
    "\n",
    "### âœ¨ **Impact of CoT Prompting**\n",
    "\n",
    "- Improves accuracy and insightfulness in responses.\n",
    "\n",
    "- Focuses on reasoning and interpretability, especially in complex scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ho8Ee15qQGw"
   },
   "source": [
    "The code below downloads an dataset which has over [1.8 million Chain of Thought examples](https://huggingface.co/datasets/kaist-ai/CoT-Collection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCE2VgOPO_L4"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset using streaming mode to avoid the file structure issue\n",
    "# Then materialize a subset of it\n",
    "dataset_stream = load_dataset(\n",
    "    \"kaist-ai/CoT-Collection\", \n",
    "    streaming=True,\n",
    "    token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    ")\n",
    "\n",
    "# Take the first 10,000 examples from the training split\n",
    "dataset = dataset_stream['train'].take(10_000) #type: ignore\n",
    "\n",
    "# Convert to a list to materialize the data\n",
    "dataset = list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 672,
     "status": "ok",
     "timestamp": 1706678952642,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "Z0DY6By96mqc",
    "outputId": "91050859-80ad-4f88-ce64-01d72dc78ea7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Article: Phytochemistry is a branch of plant b...</td>\n",
       "      <td>from plants</td>\n",
       "      <td>The article states that many medicinal and rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In this task, you are provided with an article...</td>\n",
       "      <td>Regulation</td>\n",
       "      <td>This act has all the features of Regulation. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What's the answer to that question: where did ...</td>\n",
       "      <td>Battle of Appomattox Court House</td>\n",
       "      <td>The Battle of Appomattox Court House was a bat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Extract the answer to the question from the fo...</td>\n",
       "      <td>major heart attacks</td>\n",
       "      <td>The answer is the number of major heart attack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which entity is this text about?\\n\\nRichard \"R...</td>\n",
       "      <td>Red Skelton</td>\n",
       "      <td>The text is about Red Skelton. The context des...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  Article: Phytochemistry is a branch of plant b...   \n",
       "1  In this task, you are provided with an article...   \n",
       "2  What's the answer to that question: where did ...   \n",
       "3  Extract the answer to the question from the fo...   \n",
       "4  Which entity is this text about?\\n\\nRichard \"R...   \n",
       "\n",
       "                             target  \\\n",
       "0                       from plants   \n",
       "1                        Regulation   \n",
       "2  Battle of Appomattox Court House   \n",
       "3               major heart attacks   \n",
       "4                       Red Skelton   \n",
       "\n",
       "                                           rationale  \n",
       "0  The article states that many medicinal and rec...  \n",
       "1  This act has all the features of Regulation. T...  \n",
       "2  The Battle of Appomattox Court House was a bat...  \n",
       "3  The answer is the number of major heart attack...  \n",
       "4  The text is about Red Skelton. The context des...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the list of examples to a pandas DataFrame\n",
    "dataset_df = pd.DataFrame(dataset) #type: ignore\n",
    "\n",
    "# Remove the 'task' and 'type' columns if they exist\n",
    "columns_to_remove = [col for col in ['task', 'type'] if col in dataset_df.columns]\n",
    "if columns_to_remove:\n",
    "    dataset_df = dataset_df.drop(columns=columns_to_remove)\n",
    "\n",
    "dataset_df.head()\n",
    "\n",
    "# dataset = dataset.remove_columns(['task', 'type'])\n",
    "\n",
    "# dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# dataset = dataset.select(range(10_000))\n",
    "\n",
    "# dataset = dataset.to_pandas()\n",
    "\n",
    "# dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATBYAzaXqfy1"
   },
   "source": [
    "We'll sample just 10,000 of these Chain of Thought examples and save them as a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1706678985658,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "duffkBFV69MV"
   },
   "outputs": [],
   "source": [
    "selected_examples = dataset_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1706678986795,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "kIFKE7Pd9YQj",
    "outputId": "0b37eea0-e9dc-41b9-f583-02296169cc49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 10000\n",
      "\n",
      "First example:\n",
      "{'source': 'Article: Phytochemistry is a branch of plant biochemistry primarily concerned with the chemical substances produced by plants during secondary metabolism. Some of these compounds are toxins such as the alkaloid coniine from hemlock. Others, such as the essential oils peppermint oil and lemon oil are useful for their aroma, as flavourings and spices (e.g., capsaicin), and in medicine as pharmaceuticals as in opium from opium poppies. Many medicinal and recreational drugs, such as tetrahydrocannabinol (active ingredient in cannabis), caffeine, morphine and nicotine come directly from plants. Others are simple derivatives of botanical natural products. For example, the pain killer aspirin is the acetyl ester of salicylic acid, originally isolated from the bark of willow trees, and a wide range of opiate painkillers like heroin are obtained by chemical modification of morphine obtained from the opium poppy. Popular stimulants come from plants, such as caffeine from coffee, tea and chocolate, and nicotine from tobacco. Most alcoholic beverages come from fermentation of carbohydrate-rich plant products such as barley (beer), rice (sake) and grapes (wine).\\n\\nNow answer this question: Where do some medicines and recreational drugs come from?', 'target': 'from plants', 'rationale': 'The article states that many medicinal and recreational drugs, such as tetrahydrocannabinol (active ingredient in cannabis), caffeine, morphine and nicotine come directly from plants. These are some examples of the medicines found in plants mentioned by the author. Thus it can be stated with certainty that some medicines do indeed come from plants.\\n\\nTherefore, \"from plants\" is the correct answer option to this question based on the context provided.\"'}\n",
      "\n",
      "Keys in first example:\n",
      "dict_keys(['source', 'target', 'rationale'])\n"
     ]
    }
   ],
   "source": [
    "# Check the structure of the first example\n",
    "print(\"Number of examples:\", len(selected_examples))\n",
    "print(\"\\nFirst example:\")\n",
    "print(selected_examples[0])\n",
    "print(\"\\nKeys in first example:\")\n",
    "print(selected_examples[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctCFgLemqll9"
   },
   "source": [
    "For this example we'll use OpenAI's embedding model for reliability. If you prefer to use HuggingFace to avoid costs, you can uncomment the alternative code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3834,
     "status": "ok",
     "timestamp": 1706679095210,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "qmyv-uT-EmyS"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU69WA8pq0wb"
   },
   "source": [
    "To set up a Chain of Thought prompt you will use a `FewShotPromptTemplate` as well as an example selector.\n",
    "\n",
    "In this example you'll use `MaxMarginalRelevanceExampleSelector`, but you can use any of the example selectors you learned about before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1706679404593,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "6YLFBPqMFId7"
   },
   "outputs": [],
   "source": [
    "from langchain_core.example_selectors import MaxMarginalRelevanceExampleSelector\n",
    "\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1706679442334,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "zYhYHn7NF4n_"
   },
   "outputs": [],
   "source": [
    "prefix = \"Consider the following as examples of how to reason:\"\n",
    "\n",
    "examples_template = \"\"\"Query: {source}\n",
    "\n",
    "Rationale: {rationale}\n",
    "\n",
    "Response: {target}\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"Using a similar reasoning approach, answer the users question which is delimited by triple backticks.\n",
    "\n",
    "User question: ```{input}```\n",
    "\n",
    "Take a deep breath, break down the user's query step-by-step, and provide a clear chain of thought in your response.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 364,
     "status": "ok",
     "timestamp": 1706679453143,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "nXD2hFzNH9hL"
   },
   "outputs": [],
   "source": [
    "examples_prompt = PromptTemplate(\n",
    "    input_variables=[\"source\", \"rationale\", \"target\"],\n",
    "    template=examples_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXuPx89GtMLv"
   },
   "source": [
    "There's a large number of examples to embed and add to the vector store. The below cell will take a few minutes to run. It took me ~3 minutes using the free T4 GPU provided on Google colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 186888,
     "status": "ok",
     "timestamp": 1706679667969,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "5SA1Tst3IO6J"
   },
   "outputs": [],
   "source": [
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    selected_examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    embeddings,\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS,\n",
    "    # The number of examples to produce.\n",
    "    k=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1706679667970,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "cuFE8t32IX7S"
   },
   "outputs": [],
   "source": [
    "mmr_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=examples_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1706679667970,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "shEayc6UI0o1"
   },
   "outputs": [],
   "source": [
    "query = \"\"\"Here's a short story: A lifter uses a barbell, but moves a jump rope\\\n",
    "in a wider arc. The object likely to travel further is (A) the jump rope (B) the\\\n",
    "barbell.\n",
    "\n",
    "What is the most sensical answer between \"a jump rope\" and \"a barbell\"?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1706679667970,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "KfA-c4gvJM9Q",
    "outputId": "300a3d1d-cae1-409c-acba-0f31100b73d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider the following as examples of how to reason:\n",
      "\n",
      "Query: Here's a short story: A lifter uses a barbell, but moves a jump rope in a wider arc. The object likely to travel further is (A) the jump rope (B) the barbell.\n",
      "\n",
      "What is the most sensical answer between \"a jump rope\" and  \"a barbell\"?\n",
      "\n",
      "Rationale: The statement \"a lifter uses a barbell, but moves a jump rope in a wider arc\" is comparing the two objects. \n",
      "A barbell is used for weightlifting and has 2 weights on each end of an iron rod. A jump rope is used for jumping exercises and consists of one thin cord wrapped around handles or gripped at both ends. Both are relatively light-weight objects with low mass that can be held by hand; however, because it covers more distance during exercise (i) requires less effort to move (ii) travels farther when dropped compared to the other object mentioned: the barbell which likely weighs much heavier than either choice given as answers (\"the Barbell\".\n",
      "Therefore answer @option 1(\"a jump rope\") would make sense while choosing between these two options\n",
      "\n",
      "Response: a jump rope\n",
      "\n",
      "\n",
      "Query: Here's a short story: A reed is more flexible then a stick. If bent, which object will be less likely to break? (A) reed (B) stick.\n",
      "\n",
      "What is the most sensical answer between \"reed\" and  \"stick\"?\n",
      "\n",
      "Rationale: The question asks which object will be less likely to break if bent. The story states that a reed is more flexible than a stick, and flexibility refers to the ability of an item to bend without breaking or becoming misshapen. This implies that when held at equal angles, the reed would be able hold its shape better than the stick, making it \"less likely\" to break in comparison with a stick.\n",
      "So based on this logic and context, option A (\"reed\") is the most sensical answer\n",
      "\n",
      "Response: reed\n",
      "\n",
      "\n",
      "Query: In this task, you need to answer basic science questions. For each question, several terms are intentionally masked with ***. This masking can cover very few or many of the question words. Clearly, if masked terms are important, the modified question will be impossible to answer. Please indicate the correct answer with one of the following responses: \"A\", \"B\", \"C\", \"D\" or \"E\". While \"A\"-\"D\" correspond to the answer options provided in the input, \"E\" represents \"I don't know\" for questions that do not provide enough information. Respond via \"E\" if the question is not answerable. Do not generate anything else apart from one of the following characters: 'A', 'B, 'C', 'D', 'E'.\n",
      "\n",
      "Question: A *** is an *** of which *** of *** ***?  (A) lever (B) pulley (C) wheel (D) inclined plan.\n",
      "\n",
      "Rationale: The question is unanswerable because the masked terms are essential to answering the question. The question asks \"What is an ___ of which ___ of ___ ___?\" Therefore, it asks for a specific type of object (e.g., lever, pulley or wheel). However, this information is not provided due to the masking in the question. Consequently, there are many possible answers and thus it is impossible to determine which object out of all possible objects would be correct. As a result, we do not know which answer option (\"A\", \"B\", \"C\" or \"D\") corresponds to the correct answer for this question and therefore can only respond with 'E'.\n",
      "\n",
      "Response: E\n",
      "\n",
      "\n",
      "Query: Here's a short story: If a truck and motorcycle accelerate as fast as possible, which one will be the last to reach 40 miles per hour? (A) Truck (B) Motorcycle.\n",
      "\n",
      "What is the most sensical answer between \"Truck\" and  \"Motorcycle\"?\n",
      "\n",
      "Rationale: The short story talks about two objects, a truck and a motorcycle. It states that the both accelerate as fast as possible to 40 mph. The question then asks which one will be the last one to reach 40 miles per hour.\n",
      "A truck is much heavier than a motorcycle and also has more power due to its bigger engine size compared with a motorbike's smaller engine size so it will likely take longer for it to get up to speed than for the lighter-weighted bike (with less powerful engines). That means that if accelerates \"as fast as possible\", the Truck would still have an advantage over Motorcycle because of their differences in weight/engine sizes when they are accelerating at full capacity or \"as fast as possible\".\n",
      "\n",
      "Response: Truck\n",
      "\n",
      "\n",
      "Query: Here's a short story: Glenn rolls a marble on a table and the marble moves easily. Glenn rolls a marble on the carpet and it moves slowly and not very far. The marble rolled easily on the table because there was (A) less friction (B) more friction..\n",
      "\n",
      "What is the most sensical answer between \"table\" and  \"carpet\"?\n",
      "\n",
      "Rationale: In the story, Glenn rolls a marble on a table and it moves very easily. He then rolls the same marble across a carpet and it moves slowly and not as far. This is because there was less friction between the marble and the smooth surface of the table than when rolling across rough carpet fibers. So, answer A: \"table\" makes more sense than B: \"carpet\".\n",
      "\n",
      "Response: table\n",
      "\n",
      "\n",
      "Query: Here's a short story: Dylan threw a balloon and basketball out the window. The _____ had a stronger gravity pull because of it's greater mass. (A) balloon (B) basketball.\n",
      "\n",
      "What is the most sensical answer between \"basketball\" and  \"balloon\"?\n",
      "\n",
      "Rationale: Although both objects have the same shape and are equally light, a balloon is filled with air. This means that it has greater mass than an empty ball of rubber in this case, making \"balloon\" the answer here.\n",
      "\n",
      "Response: balloon\n",
      "\n",
      "\n",
      "Query: Answer the question depending on the context.\n",
      "Context: On the uneven bars, the gymnast performs a routine on two horizontal bars set at different heights. These bars are made of fiberglass covered in wood laminate, to prevent them from breaking. In the past, bars were made of wood, but the bars were prone to breaking, providing an incentive to switch to newer technologies. The width and height of the bars may be adjusted. In the past, the uneven parallel bars were closer together. They've been moved increasingly further apart, allowing gymnasts to perform swinging, circling, transitional, and release moves, that may pass over, under, and between the two bars. At the Elite level, movements must pass through the handstand. Gymnasts often mount the Uneven Bars using a springboard, or a small mat. Chalk and grips (a leather strip with holes for fingers to protect hands and improve performance) may be used while doing this event. The chalk helps take the moisture out of gymnast's hands to decrease friction and prevent rips (tears to the skin of the hands), dowel grips help gymnasts grip the bar.;\n",
      "Question: What are the uneven bars?;\n",
      "Answer:\n",
      "\n",
      "Rationale: The context describes the \"uneven bars\", also known as asymmetric parallel bars, and explains that they consist of two horizontal bars set at different heights. \n",
      "Therefore, the answer is: \"two horizontal bars set at different heights\".\n",
      "\n",
      "Response: two horizontal bars set at different heights\n",
      "\n",
      "\n",
      "Using a similar reasoning approach, answer the users question which is delimited by triple backticks.\n",
      "\n",
      "User question: ```Here's a short story: A lifter uses a barbell, but moves a jump ropein a wider arc. The object likely to travel further is (A) the jump rope (B) thebarbell.\n",
      "\n",
      "What is the most sensical answer between \"a jump rope\" and \"a barbell\"?\n",
      "```\n",
      "\n",
      "Take a deep breath, break down the user's query step-by-step, and provide a clear chain of thought in your response.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mmr_prompt.format(input=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17532,
     "status": "ok",
     "timestamp": 1706679778282,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "KFw8vxigJTjW",
    "outputId": "ce515fde-f96a-45b8-eb8f-0728907b3161"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I canâ€™t share step-by-step chain-of-thought, but hereâ€™s a concise answer with a brief justification.\\n\\nMost sensical answer: a jump rope.\\n\\nReason: The jump rope is swung in a wider arc, so its tip travels a longer path than a barbell moved in the same motion. The barbell is heavier and generally travels a shorter distance in the described action.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=open_ai_model, temperature=0.0)\n",
    "\n",
    "llm_chain = mmr_prompt | llm | StrOutputParser()\n",
    "\n",
    "llm_chain.invoke({\"input\":query})\n",
    "\n",
    "# for chunk in llm_chain.stream({\"input\":query}):\n",
    "#   print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 422,
     "status": "ok",
     "timestamp": 1706679798740,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "FftbmTl6LgQ-"
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "Given the fact that: Inhaling, or breathing in, increases the size of the chest, \\\n",
    " which decreases air pressure inside the lungs. Answer the question: If Mona is \\\n",
    " done with a race and her chest contracts, what happens to the amount of air \\\n",
    " pressure in her lungs increases or decreases?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21125,
     "status": "ok",
     "timestamp": 1706679821198,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "OZrMB0i0Lzif",
    "outputId": "8f82828f-309a-4f05-acde-7a7500b1240e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I canâ€™t share canâ€™t share my step my step-by-step-by-step chain-of-th chain-of-thoughtought, but, but hereâ€™s hereâ€™s a concise answer with a concise answer with a brief a brief justification.\n",
      "\n",
      " justification.\n",
      "\n",
      "Answer:Answer: increases\n",
      "\n",
      " increases\n",
      "\n",
      "Brief justificationBrief justification:\n",
      "-:\n",
      "- When the chest contracts When the chest contracts, its volume, its volume decreases decreases. \n",
      ". \n",
      "- A decrease in- A decrease in lung volume raises lung volume raises the the air pressure inside the air pressure inside the lungs relative to lungs relative to the outside air the outside air, causing air to, causing air to be expelled. \n",
      " be expelled. \n",
      "- This is the- This is the opposite of opposite of inhalation inhalation, which, which expands the chest expands the chest and lowers and lowers intr intrapulmonaryapulmonary pressure pressure to draw to draw in air. in air."
     ]
    }
   ],
   "source": [
    "for chunk in llm_chain.stream({\"input\":query}):\n",
    "  print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJFjvwxWCwvX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNUpicQNfihPPdnwto66QtM",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
