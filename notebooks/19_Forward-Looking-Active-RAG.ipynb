{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48475,
     "status": "ok",
     "timestamp": 1706853315663,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "sHgv4EQ7hBxA",
    "outputId": "065980d4-73f6-4bef-decf-be5a1f981317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Your OpenAI API Key:路路路路路路路路路路\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")\n",
    "open_ai_model = \"gpt-5-nano-2025-08-07\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvaIosUctu5H"
   },
   "source": [
    "# FLARE\n",
    "\n",
    "### What is FLARE?\n",
    "\n",
    "[FLARE](https://arxiv.org/pdf/2305.06983.pdf) stands for `Forward-Looking Active REtrieval` augmented generation.  This is a method that was introduced in 2023 in a paper titled \"Active Retrieval Augmented Generation\" by Zhengbao Jiang, et al. The original implementation can be found in [the paper's GitHub repo](https://github.com/jzbjyb/FLARE), which the LangChain implementation draws heavy inspiration from.\n",
    "\n",
    "\n",
    "##  **Why FLARE Matters**\n",
    "\n",
    "- Enhances large language models by fetching info from external sources while generating content.\n",
    "\n",
    "- Tackles the issue of models making stuff up or being factually wrong.\n",
    "\n",
    "- Aims for outputs that are more accurate and well-informed.\n",
    "\n",
    "##  **FLARE's Approach:**\n",
    "- Starts by answering a question.\n",
    "\n",
    "- Looks up relevant documents when hitting \"uncertain\" tokens (based on log probs).\n",
    "\n",
    "- Uses these documents to keep generating.\n",
    "\n",
    "- Repeats the process until done.\n",
    "\n",
    "- Highlights uncertain tokens and forms questions for LLM to find answers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jzbjyb/FLARE/main/res/flare.gif\">\n",
    "\n",
    "##  **How FLARE Stands Out from Traditional RAG:**\n",
    "\n",
    "- Traditional RAG fetches info once based on the input, then generates an output.\n",
    "\n",
    "- Can be restrictive for long texts.\n",
    "\n",
    "- FLARE keeps deciding **when and what to fetch** during the generation.\n",
    "\n",
    "- Predicts the upcoming sentence to guide future content queries.\n",
    "\n",
    "- If a sentence has low-confidence tokens, it fetches relevant info and rewrites the sentence.\n",
    "\n",
    "\n",
    "###  **FLARE Prompt Structure:**\n",
    "\n",
    "- Guides language models to create search queries during answer generation.\n",
    "\n",
    "- For example: \"The colors on the flag of Ghana mean... Red stands for [Search(Ghana flag red meaning)] the blood of martyrs...\"\n",
    "\n",
    "###  **Direct FLARE Method:**\n",
    "\n",
    "- Uses the next sentence generated by the language model for retrieval decisions.\n",
    "\n",
    "- Accepts the sentence if the model is confident.\n",
    "\n",
    "- If unsure, transforms the sentence into search queries to get more info, then revises the sentence.\n",
    "\n",
    "###  **Setting Up the FLARE Chain:**\n",
    "\n",
    "1. **LLM for Answers**: Needs to return log probabilities to spot uncertain tokens. Recommended: OpenAI wrapper (not ChatOpenAI wrapper, as it doesn't return logprobs).\n",
    "\n",
    "2. **LLM for Hypothetical Questions**: Can be any model. In this case, we use ChatOpenAI for its speed and cost-effectiveness.\n",
    "\n",
    "3. **Retriever**: Any retriever works. Here, the SERPER search engine is used for cost efficiency.\n",
    "\n",
    "####  **Key Parameters in FLARE:**\n",
    "\n",
    "1. **`max_generation_len`**: Caps the number of tokens generated before checking for uncertainties.\n",
    "\n",
    "2. **`min_prob`**: Tokens generated with a probability below this threshold are marked as uncertain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2903,
     "status": "ok",
     "timestamp": 1706853695804,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "mEoXA-kQpdGz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mo/IADT_local/25_26/AI/Prompt_Engineering_LangChain/Prompt_Engineering_LangChain/.venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "/Users/mo/IADT_local/25_26/AI/Prompt_Engineering_LangChain/Prompt_Engineering_LangChain/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_classic.chains import FlareChain\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.callbacks.manager import AsyncCallbackManagerForRetrieverRun, CallbackManagerForRetrieverRun\n",
    "\n",
    "# We set this so we can see what exactly is going on\n",
    "from langchain_core.globals import set_verbose\n",
    "\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoNiUTL8ahup"
   },
   "source": [
    "## Same pattern as we saw in the RAG lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix SSL certificate verification (macOS issue)\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 6238,
     "status": "ok",
     "timestamp": 1706853732867,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "QA126pbpu0UK"
   },
   "outputs": [],
   "source": [
    "# instantiate llm\n",
    "llm = ChatOpenAI(logprobs=True) # logprobs=True for FLARE\n",
    "# logprobs: Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.\n",
    "\n",
    "#instantiate embeddings model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# fetch docs from arxiv, in this case it's the FLARE paper\n",
    "docs = ArxivLoader(query=\"2305.06983\", load_max_docs=2).load()\n",
    "\n",
    "# instantiate text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=16)\n",
    "\n",
    "# split the document into chunks\n",
    "doc_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# create vector store consisting of embeddigs for document\n",
    "vector_store = FAISS.from_documents(doc_chunks, embedding=embeddings)\n",
    "\n",
    "vector_store_retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtSrDBBSa0i5"
   },
   "source": [
    "# Set up the FLARE chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 533,
     "status": "ok",
     "timestamp": 1706853752284,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "4DZGInhUwgp_"
   },
   "outputs": [],
   "source": [
    "flare = FlareChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vector_store_retriever,\n",
    "    max_generation_len=1024,\n",
    "    min_prob=0.7,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OE-f2-xma2q0"
   },
   "source": [
    "# Start generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8074,
     "status": "ok",
     "timestamp": 1706853871805,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "DQ1oAcawL3RM",
    "outputId": "d6bf7f8e-c389-4d2d-be3f-1b5b7a5c1d37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new FlareChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mCurrent Response: \u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mGenerated Questions: ['What are the various contexts in which FLARE can be interpreted?', 'What are some examples of how FLARE can be interpreted or defined?', 'What is FLARE commonly used to describe in the fashion industry?', 'What specific industry might use the term \"FLARE\"?', 'What could FLARE be used to describe in the fashion industry?', 'Can you provide more information or details in order to help me understand the context better?', 'Can you provide more context so I can give you a _________?']\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mCurrent Response:  FLARE stands for Forward-Looking Active Retrieval Augmented Generation. It is a method that achieves superior or competitive performance on long-form knowledge-intensive generation tasks by effectively retrieving necessary information throughout the generation process.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mGenerated Questions: ['What is FLARE and how does it revolutionize the generation of knowledge-intensive content through active retrieval techniques?', 'Is FLARE worth _________________________?']\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user_input': 'What is FLARE?',\n",
       " 'response': 'FLARE stands for Forward-Looking Active Retrieval Augmented Generation. It is a method that achieves superior or competitive performance on long-form knowledge-intensive generation tasks by effectively retrieving necessary information throughout the generation process. '}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flare.invoke(\"What is FLARE?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 354683,
     "status": "ok",
     "timestamp": 1706854258903,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "KvaLunCFL4ug",
    "outputId": "5073742a-00f7-4caa-a081-44c8f6d678d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new FlareChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mCurrent Response: \u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mGenerated Questions: ['What steps typically make up a FLARE pipeline?', 'What are the key steps in a FLARE pipeline that help in assessing the effectiveness and performance of the machine learning solution?', 'What is the significance of the steps in a FLARE pipeline for developing an effective machine learning solution?']\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mCurrent Response:  In a FLARE pipeline, the steps typically involve using forward-looking active retrieval augmented generation. This includes retrieving additional information only when the language models do not have the necessary knowledge to avoid unnecessary or inappropriate responses, using reinforcement learning or supervised training where multiple queries can be triggered before generation to improve retrieval quality, and evaluating performance on diverse tasks involving generating long outputs such as multihop QA and commonsense reasoning. The combination of text-based retrievers and potentially a browser can enhance the effectiveness of the FLARE method.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mGenerated Questions: ['What specific process or sequence of actions is involved in a FLARE pipeline?', 'What approach is used in a FLARE pipeline that combines retrieval with generation to improve performance on tasks like multihop QA and commonsense reasoning?', 'What approach in a FLARE pipeline involves combining text-based retrievers with reinforcement learning or supervised training?', 'What tasks is the FLARE approach evaluated on to improve retrieval quality?', \"How can a browser component enhance the effectiveness of the FLARE method and ultimately enhance what aspect of the system's functionality?\"]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user_input': 'What are the steps in a FLARE pipeline?',\n",
       " 'response': 'In a FLARE pipeline, the steps typically involve using forward-looking active retrieval augmented generation. This includes retrieving additional information only when the language models do not have the necessary knowledge to avoid unnecessary or inappropriate responses, using reinforcement learning or supervised training where multiple queries can be triggered before generation to improve retrieval quality, and evaluating performance on diverse tasks involving generating long outputs such as multihop QA and commonsense reasoning. The combination of text-based retrievers and potentially a browser can enhance the effectiveness of the FLARE method. '}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flare.invoke(\"What are the steps in a FLARE pipeline?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "a9K3HpCxMIJY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new FlareChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mCurrent Response: \u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mGenerated Questions: ['What components are typically included in a retrieval instruction in FLARE?', 'What are some components that a typical retrieval instruction in FLARE may involve specifying?', 'What factors determine the specific format of a retrieval instruction in FLARE?']\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mCurrent Response:  In FLARE, a typical retrieval instruction prompts the LM to generate search queries when necessary while generating the answer using retrieval-encouraging instructions, denoted as FLAREinstruct. These instructions guide the model on when and what information to retrieve to enhance the quality of the generated output.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mGenerated Questions: ['What is involved in a typical retrieval instruction in FLARE?', 'What guides the Language Model (LM) on when and what information to retrieve in FLARE to enhance the quality of the generated output?', 'How does FLARE ensure that generated outputs include essential details by using retrieval instructions?', 'What result does using retrieval instructions in FLARE aim to achieve, specifically in terms of the quality and robustness of the response?', 'How can FLARE guide the LM in generating search queries and retrieving relevant information to enhance the quality of the generated output?']\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user_input': 'What does a typical retrieval instruction look like in FLARE?',\n",
       " 'response': 'In FLARE, a typical retrieval instruction prompts the LM to generate search queries when necessary while generating the answer using retrieval-encouraging instructions, denoted as FLAREinstruct. These instructions guide the model on when and what information to retrieve to enhance the quality of the generated output. '}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flare.invoke(\"What does a typical retrieval instruction look like in FLARE?\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1JDuyfKY67dnYM5IUlxZgUA0m4u7cFdst",
     "timestamp": 1697211613393
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
