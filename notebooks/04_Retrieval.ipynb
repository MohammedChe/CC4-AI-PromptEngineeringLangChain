{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1706242390622,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "a15mWUKivAZL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4085,
     "status": "ok",
     "timestamp": 1706242394703,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "sjmRd93XvAmj",
    "outputId": "e4e8e34f-508d-44e5-f755-f391688909c5"
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1706242796237,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "-c2oiPNTwNKk",
    "outputId": "3ff6b38b-5b49-4000-c675-fec0d88492a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of songs in the file is: 102\n",
      "The total number of words in the file is: 65688\n"
     ]
    }
   ],
   "source": [
    "filename = \"../data/kdot.txt\"\n",
    "word_count = 0\n",
    "song_count = 0\n",
    "\n",
    "# Open the file with UTF-8 encoding to avoid Windows cp1252 decode errors\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if \"TRACK:\" in line:\n",
    "            song_count += 1\n",
    "        words = line.split()\n",
    "        word_count += len(words)\n",
    "\n",
    "print(f\"The total number of songs in the file is: {song_count}\")\n",
    "print(f\"The total number of words in the file is: {word_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuzRSeWYwOn5"
   },
   "source": [
    "# üîç **Retrieval in LangChain Explained**\n",
    "\n",
    "<img src=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=1100&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=675f55e100bab5e2904d27db01775ccc\">\n",
    "\n",
    "### üåê **Basic Concept**\n",
    "\n",
    "Retrieval is like gathering resources to enhance an essay, helping language models access up-to-date, relevant information beyond their built-in knowledge.\n",
    "\n",
    "üí° **Advantages**:\n",
    "   - Adds new, fresh information.\n",
    "   - Makes responses more relevant and informed.\n",
    "\n",
    "üìö **Document Loaders**:\n",
    "   - Function as \"specialized librarians.\"\n",
    "   - Organize content from various sources for language models.\n",
    "\n",
    "üìÑ **Text Loader Fundamentals**:\n",
    "   - Simple process: Converts text files into a usable format for language models.\n",
    "\n",
    "üéØ **Presentation Style**:\n",
    "   - Brief and informative, ideal for a concise summary.\n",
    "\n",
    "[LangChain Learn: RAG](https://docs.langchain.com/oss/python/langchain/rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 805,
     "status": "ok",
     "timestamp": 1706243183108,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "ECz7yiNBwZKi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cherbatjim\\Desktop\\AI\\CC4-AI-PromptEngineeringLangChain\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "c:\\Users\\cherbatjim\\Desktop\\AI\\CC4-AI-PromptEngineeringLangChain\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error loading ../data/kdot.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cherbatjim\\Desktop\\AI\\CC4-AI-PromptEngineeringLangChain\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py:43\u001b[39m, in \u001b[36mTextLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m.file_path, encoding=\u001b[38;5;28mself\u001b[39m.encoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m         text = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\encodings\\cp1252.py:23\u001b[39m, in \u001b[36mIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'charmap' codec can't decode byte 0x9d in position 92126: character maps to <undefined>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextLoader\n\u001b[32m      4\u001b[39m loader = TextLoader(\u001b[33m\"\u001b[39m\u001b[33m../data/kdot.txt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m kendrick_lyrics = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cherbatjim\\Desktop\\AI\\CC4-AI-PromptEngineeringLangChain\\.venv\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:43\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     38\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into `Document` objects.\u001b[39;00m\n\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m        The documents.\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cherbatjim\\Desktop\\AI\\CC4-AI-PromptEngineeringLangChain\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py:56\u001b[39m, in \u001b[36mTextLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     54\u001b[39m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Error loading ../data/kdot.txt"
     ]
    }
   ],
   "source": [
    "# https://docs.langchain.com/oss/python/integrations/retrievers\n",
    "# https://docs.langchain.com/oss/python/integrations/document_loaders\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"../data/kdot.txt\" encoding=\"utf-8\")\n",
    "kendrick_lyrics = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1706243186423,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "p7ghrb20wbXO",
    "outputId": "27b4c969-6576-4eb2-9be5-cba4d6f9e5c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(kendrick_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 169,
     "status": "ok",
     "timestamp": 1706243195356,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "cHJNMIwdwhAv",
    "outputId": "c834dce5-4377-4541-98cf-19b2a1f88715"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(kendrick_lyrics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onMQEXT7wjMc"
   },
   "source": [
    "# üîÑ **Document Loaders in LangChain**:\n",
    "\n",
    "üìã **Wide Selection**: Numerous document loaders available. Check the [documentation](https://docs.langchain.com/oss/python/integrations/document_loaders) for a full list.\n",
    "\n",
    "üë£ **Usage Steps**:\n",
    "   1. Choose a Document Loader from LangChain.\n",
    "   2. Create an instance of the Document Loader.\n",
    "   3. Employ its `load()` method to convert files into LangChain documents.\n",
    "\n",
    "### üõ†Ô∏è **Role of Document Transformers**\n",
    "\n",
    "üìê **Customization for Models**: Adjust documents to suit your model's requirements, like trimming lengthy texts.\n",
    "\n",
    "### ‚úÇÔ∏è **Understanding Text Splitters**\n",
    "\n",
    "üî¢ **Function**: Divide long texts into smaller, coherent segments.\n",
    "\n",
    "üîó **Goal**: Keep related text together, fitting within the model's capacity.\n",
    "\n",
    "### üß© **Using `RecursiveCharacterTextSplitter`**\n",
    "\n",
    "üîÑ **Methodology**:\n",
    "   - Intelligently splits texts using multiple separators.\n",
    "\n",
    "   - Recursively adjusts if segments are too large.\n",
    "\n",
    "   - Ensures all parts are appropriately sized.\n",
    "\n",
    "### üåü **Key Aspects of Splitting**\n",
    "\n",
    "   - Chooses optimal separators for division.\n",
    "\n",
    "   - Continually splits large chunks.\n",
    "\n",
    "   - Balances chunk size by characters or tokens.\n",
    "\n",
    "   - Maintains some overlap for context.\n",
    "\n",
    "   - Tracks chunk starting points if needed.\n",
    "\n",
    "üéØ **Presentation Style**\n",
    "\n",
    "   - Focused on essential steps and features, great for a concise summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 138,
     "status": "ok",
     "timestamp": 1706243341736,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "4gItthdswrME"
   },
   "outputs": [],
   "source": [
    "# https://docs.langchain.com/oss/python/integrations/splitters\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap = 50,\n",
    "    length_function = len,\n",
    "    add_start_index = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1706243354577,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "E00tIW5ox0n9"
   },
   "outputs": [],
   "source": [
    "texts = text_splitter.split_documents(kendrick_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 157,
     "status": "ok",
     "timestamp": 1706243359603,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "mf0Nj7YMyJjg",
    "outputId": "ccef7959-c742-4cb7-f951-2490bd1bb225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='---\n",
      "ALBUM: Overly Dedicated\n",
      "TRACK: Growing Apart\n",
      "LYRICS:' metadata={'source': '../data/kdot.txt', 'start_index': 0}\n",
      "page_content='\"Where are we going? Why are we slowing down?\n",
      "Where are you going? We should be growing now\n",
      "Smoke to it nigga, smoke to it nigga, oh\n",
      "Where are we going? We should be slowing down\n",
      "Where are you going? We should be growing now\n",
      "That‚Äôs what she said to me, that place I used to call home\n",
      "Is just a bed to me\n",
      "And we don‚Äôt even sleep, neighbors can hear her weep\n",
      "Meanwhile I‚Äôm in these streets with everybody, I‚Äôm trying to get it\n",
      "And she know they got me, I watch her feelings watch me\n",
      "As they staring with the saddest eyes of loneliness\n",
      "Look each other in the face and barely blink\n",
      "I tried to make it right, but the pen ran out of ink\n",
      "So if my letters don‚Äôt reach you, I hope these lyrics in sync\n",
      "That‚Äôs what it said to me\n",
      "But the place I call ambition now dead to me\n",
      "Gone and forgotten, I‚Äôm off track like Dale Earnhardt\n",
      "My liver rotten, alcoholic tripping\n",
      "Fucking bad bitches and they got bad intentions\n",
      "Club night, guys and dolls\n",
      "Balling out but I'm 'bout to drop the ball' metadata={'source': '../data/kdot.txt', 'start_index': 58}\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsF1E9sUyhL-"
   },
   "source": [
    "# üåê **Text Embeddings Overview**\n",
    "\n",
    "üî¢ **Functionality**: Converts documents into numerical vectors in LangChain.\n",
    "\n",
    "ü§ù **Similarity Measure**: Vectors that are closer indicate more similar texts.\n",
    "\n",
    "üîç **Application**: Quickly identify documents with similar topics or content.\n",
    "\n",
    "üéØ **Presentation Style**: Concise and clear, ideal for slides or quick explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1706243411934,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "cIvaIq2xywMw"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wn6N5TfLq1Js"
   },
   "source": [
    "# üõ†Ô∏è **Creating a Vector Store Retriever**\n",
    "\n",
    "1. **Load Documents**: Utilize a document loader for initial document retrieval.\n",
    "\n",
    "2. **Split Texts**: Break down documents into smaller sections with a text splitter.\n",
    "\n",
    "3. **Embedding Conversion**: Apply an embedding model to transform text chunks into vectors.\n",
    "\n",
    "4. **Vector Store Creation**: Compile these vectors into a vector store.\n",
    "\n",
    "üîç **Outcome**: Your vector store is now set up to search and retrieve texts by content.\n",
    "\n",
    "[Documentation](https://docs.langchain.com/oss/python/integrations/vectorstores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 3941,
     "status": "ok",
     "timestamp": 1706243462944,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "tMTITqujy-Ny"
   },
   "outputs": [],
   "source": [
    "# https://docs.langchain.com/oss/python/integrations/vectorstores\n",
    "# https://docs.langchain.com/oss/python/langchain/knowledge-base#faiss\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(documents=texts, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ukx_BoFuqWgh"
   },
   "source": [
    "# üîé **Vector Store as a Retriever**\n",
    "\n",
    "1. **Search Engine Role**: The vector store functions like a document search engine.\n",
    "\n",
    "2. **Similarity Searches**: Find documents similar to your provided text.\n",
    "\n",
    "3. **Customization Options**: Specify match selectivity and desired number of top results.\n",
    "\n",
    "‚ú® **Functionality**: Use `similarity_search` to pinpoint documents closely matching your specified text, with flexibility in refining search parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1706243566964,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "JoBei3GpskW8",
    "outputId": "c7e40b7a-2b25-45ec-edc5-10d1d628c6fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='d324a639-5f8f-4c69-a31c-e8716e5e5477', metadata={'source': '../data/kdot.txt', 'start_index': 206250}, page_content=\"I pray my experience helps you\\nAs for me I'm tryna sort it out\\nSearching for loopholes in my bruised soul\\nBut who knows?\\nI just need a little space to breathe\\nI know perception is key, so I am king\\nThe other side has never mortified my mortal mind\\nThe borderline between insanity is Father Time\\nI fall behind my skeleton, they tell me that I'm blind\\nI know that I'm intelligent, my confidence just died\\nCarpe diem, seize the day, I can't compromise\\nA tapeworm couldn't cure this gluttonous appetite\\nA couple trinkets, they seein' me as I pacify\\nBut couldn't fathom the meaning of seein' sacrifice\\nI'm passin' lives on the daily, maybe I'm losing faith\\nGenocism and capitalism just made me hate\\nCorrectionals and these private prisons gave me a date\\nProfessional dream killers reason why I'm awake\\nI'm sleepwalkin', I'm street stalkin', I'm outta place\\nReinforcing this heat barking, these are the brakes\\nBefore I blink, do I see me before them pearly gates?\\nOr this is just a mirage or a facade\\nWait\"),\n",
       " Document(id='877468f9-74c2-4a0a-9910-8f767d1b0e74', metadata={'source': '../data/kdot.txt', 'start_index': 201171}, page_content='\"So I met this young lady, ya know, when I went back home. I was tryna spit my best game to her, you know, give her my best cap, but she kept on snapping her fingers like *snaps*. I said \"\"You know what, girl? You crazy.\"\" Then she asked me what he said...(What did the Asian say?) \"\"A peace of mind\"\"\\nThat\\'s what the Asian said‚ÄîI need a divine\\nIntervention was his religion and I was surprised\\nHim believing in Buddha, me believing in God\\nAsked him what are you doing, he said \"\"taking my time\\nMeditation is a must, it don\\'t hurt if you try\\nSee you thinking too much, plus you too full of yourself\\nWorried about your career, you ever think of your health?\"\"(What did the Indian say?) \"\"A piece of land\"\"\\nThat\\'s what the Indian said‚ÄîI needed the man\\nTelling me longevity is in the dirt\\nShould buy some property first\\nShould profit a better dollar with generational perks\\nEquity at his best, really, you should invest\\nThese tangible things expire, don\\'t you expect'),\n",
       " Document(id='39beafec-e60c-4eab-8160-7f0e5999cfa0', metadata={'source': '../data/kdot.txt', 'start_index': 320804}, page_content=\"Celebrate new life when it come back around\\nThe purpose is in the lessons we learnin' now\\nSacrifice personal gain over everything\\nJust to see the next generation better than ours\\nI wasn't perfect, the skin I was in had truly suffered\\nTemptation, impatience, everything that the body nurtures\\nI felt the good, I felt the bad, and I felt the worry\\nBut all-in-all, my productivity had stayed urgent\\nFace your fears, always knew that I would make it here\\nWhere the energy is magnified and persevered\\nConsciousness is synchronized and crystal-clear\\nEuphoria is glorified and made His\\nReflectin' on my life and what I done\\nPaid dues, made rules, change outta love\\nThem same views made schools change curriculums\\nBut didn't change me starin' down the barrel of the gun\\nShould I feel resentful I didn't see my full potential?\\nShould I feel regret about the good that I was into?\\nEverything is everything, this ain't coincidental\\nI woke up that morning with more heart to give you\"),\n",
       " Document(id='118a5064-abd0-48df-82d5-0a9ec1b47d1b', metadata={'source': '../data/kdot.txt', 'start_index': 987}, page_content='Balling out but I\\'m \\'bout to drop the ball\\nI‚Äôm calling out for help, my engineer called\\nTold me come to the studio, I wanna tell him‚Äì \"\"Nah\"\"\\nThat‚Äôs what he said to me\\nBut that place we call Heaven unfair to me, only 144 can go\\nSeven billion people on planet Earth today\\nIs there something that I don‚Äôt know\\nI know to follow Him rather following people\\nOr follow vanity cause that means I‚Äôm following evil\\nGuess I‚Äôm following evil, I should follow cathedral\\nBlessings I need but live like I don‚Äôt need you\\nSo in conclusion\\nWe all seem to stumble, planning our own demise\\nForgetting the big picture and making it wallet size\\nSo to what is important in my life, I apologize\\nI promise to stay faithful, focused and sanctified\\nWe all get distracted, the question is\\nWould you bounce back or bounce backwards?\\nWould you not know how to act or take action?\\nIt‚Äôs just a part of life and if your vision‚Äôs impaired, you probably lose it all tonight\"\\n---')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How can I practice mindfulness if I am always so busy and distracted?\"\n",
    "\n",
    "vectorstore.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5quhR5asvju"
   },
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "executionInfo": {
     "elapsed": 9737,
     "status": "ok",
     "timestamp": 1706243770715,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "BwZKC0Yzs3ta",
    "outputId": "e370cd9f-dbee-4e61-f9da-ec9b095ad92c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Grief, fear, envy, and desire stem from unresolved trauma and fear inside you. They spread like weeds when pain goes unaddressed. This dynamic is central to Kendrick Lamar's DAMN., which frames these emotions as outcomes of fear-driven wounds.\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_classic.chains import RetrievalQA\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "Use the following pieces of context to answer the question at the end. Cite the Album name that helped shape your answer.\n",
    "\n",
    "If you don't know the answer, just say 'Ah snap homie, I ain't gonna front. I don't know.`, don't try to make up an answer.\n",
    "\n",
    "Use three sentences maximum, relevant analogies, and keep the answer as concise as possible.\n",
    "\n",
    "Use the active voice, and speak directly to the reader using concise language.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano-2025-08-07\", temperature=0)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "\n",
    "query = \"What do grief, fear, envy, and desire stem from?\"\n",
    "\n",
    "\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "result['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDhZM1JjCZ_a"
   },
   "source": [
    "# üõ†Ô∏è **Using LCEL for Retrieval**\n",
    "\n",
    "1. **Integrate Context and Question**: The prompt template includes placeholders for context and question.\n",
    "\n",
    "2. **Preliminary Setup**\n",
    "   - Set up a retriever with an in-memory store for document retrieval.\n",
    "\n",
    "   - Runnable components can be chained or run separately.\n",
    "\n",
    "3. **RunnableParallel for Input Preparation**\n",
    "\n",
    "   - Use `RunnableParallel` to combine document search results and the user's question.\n",
    "\n",
    "   - `RunnablePassthrough` passes the user's question unchanged.\n",
    "\n",
    "4. **Workflow Steps**\n",
    "\n",
    "   - **Step 1**: Create `RunnableParallel` with two entries: 'context' (document results) and 'question' (user's original query).\n",
    "\n",
    "   - **Step 2**: Feed the dictionary to the prompt component, which constructs a prompt using the user's question and retrieved documents.\n",
    "\n",
    "   - **Step 3**: Model component evaluates the prompt with OpenAI LLM\n",
    "\n",
    "   - **Step 4**: `Output_parser` transforms response into a readable Python string.\n",
    "\n",
    "üîÑ **End-to-End Process**: From document retrieval and prompt creation to model evaluation and output parsing, the flow seamlessly integrates various components for an effective LLM-driven response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "executionInfo": {
     "elapsed": 4415,
     "status": "ok",
     "timestamp": 1706243913131,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "WQe5ZfQtCWAq",
    "outputId": "d7f71be1-0c1f-4629-d9e7-d8a145ff6246"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x131270e30>, search_kwargs={}),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"\\n\\nUse the following pieces of context to answer the question at the end. Cite the Album name that helped shape your answer.\\n\\nIf you don't know the answer, just say 'Ah snap homie, I ain't gonna front. I don't know.`, don't try to make up an answer.\\n\\nUse three sentences maximum, relevant analogies, and keep the answer as concise as possible.\\n\\nUse the active voice, and speak directly to the reader using concise language.\\n{context}\\n\\nQuestion: {question}\\n\\nHelpful Answer:\\n\\n\")\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x131a5ac30>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x131a593d0>, root_client=<openai.OpenAI object at 0x131a5b350>, root_async_client=<openai.AsyncOpenAI object at 0x131a5ac00>, model_name='gpt-5-nano-2025-08-07', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": vectorstore.as_retriever(), \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | QA_CHAIN_PROMPT | llm | output_parser\n",
    "\n",
    "chain.invoke(query)\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Msyls1DPD-MS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '',\n",
       " 'question': 'What is hope?',\n",
       " 'context': [Document(id='5b7c9810-e00c-4a32-9987-92499506a345', metadata={'source': '../data/kdot.txt', 'start_index': 237906}, page_content='I need some water\\nSomething came over me\\nWay too hot to simmer down, might as well overheat\\nToo close to comfort\\nAs blood rush my favorite vein\\nHeartbeat racing like a junkie\\'s\\nI just need you to want me\\nAm I asking too much?\\nLet me put the head in\\nOoh, I don\\'t want more than that; girl, I respect the cat\\nI promise, just a touch\\nLet me put the head in\\nIf it\\'s okay; she said, \"\"It\\'s okay\"\"I wake in the morning, my head spinning from the last night\\nBoth in a trance, feelings are dead‚Äîwhat a fast life\\nManager called, the lobby call is :\\nDid this before, promised myself I\\'d be a hour early\\nRoom full of clothes, bag full of money: call it loose change\\nFumbled my jewelry, \\nK, I lost a new chain\\nHop on the bird, hit the next city for another M\\nTake me a nap then do it again\\nWe all woke up, tryna tune to the daily news\\nLooking for confirmation, hoping election wasn\\'t true\\nAll of us worried, all of us buried, in our feelings deep\\nNone of us married to his proposal, make us feel cheap'),\n",
       "  Document(id='c7154ea8-c71b-460e-9b8e-e668a35876f2', metadata={'source': '../data/kdot.txt', 'start_index': 322705}, page_content='And to my neighborhood, let the good prevail\\nMake sure them babies and the leaders outta jail\\nLook for salvation when troubles get real\\n\\'Cause you can\\'t help the world until you help yourself\\nAnd I can\\'t blame the hood the day that I was killed\\nY\\'all had to see it, that\\'s the only way to feel\\nAnd though my physical won\\'t reap the benefits\\nThe energy that carry on emits still\\nI want you\\n\"\\n---'),\n",
       "  Document(id='1978f905-3319-4b88-93d9-da67d3230661', metadata={'source': '../data/kdot.txt', 'start_index': 302648}, page_content='The struggle for the right side of history\\nIndependent thought is like an eternal enemy\\nCapitalists posing as compassionates be offending me\\nYeah, suck my dick with authenticity\\nYeah, Tupac dead, gotta think for yourself\\nYeah, heroes looking for the villains to help\\nI never been sophisticated, saving face\\nBeing manipulative, such an acquired taste\\nI rubbed elbows with people that was for the people\\nThey all greedy, I don\\'t care for no public speaking\\nAnd they like to wonder where I\\'ve been\\nProtecting my soul in the valley of silence\\n\"\\n---'),\n",
       "  Document(id='6f99f424-c95b-47c1-a442-d1114d8ae940', metadata={'source': '../data/kdot.txt', 'start_index': 164892}, page_content='On the dead homies\\nOn the dead homies\\nI remember you was conflicted\\nMisusing your influence\\nSometimes I did the same\\nAbusing my power full of resentment\\nResentment that turned into a deep depression\\nFound myself screaming in a hotel room\\nI didn\\'t want to self-destruct\\nThe evils of Lucy was all around me\\nSo I went running for answers\\nUntil I came home\\nBut that didn\\'t stop survivors guilt\\nGoing back and forth\\nTrying to convince myself the stripes I earned\\nOr maybe how A- my foundation was\\nBut while my loved ones was fighting a continuous war\\nBack in the city\\nI was entering a new one\\n\"\\n---')],\n",
       " 'answer': \"Hope is the stubborn flame that keeps you moving through hardship, the belief that tomorrow can be better even when today hurts. It‚Äôs a light you carry forward, an energy that persists beyond a person and guides others. This idea shaped my answer, drawn from Kendrick Lamar's To Pimp a Butterfly.\"}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(\n",
    "    llm, QA_CHAIN_PROMPT\n",
    ")\n",
    "retrieval_chain = create_retrieval_chain(vectorstore.as_retriever(), combine_docs_chain)\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": \"\", \"question\": \"What is hope?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Explicit retrieve-then-LLM example,\n",
    "\n",
    "The cell below performs an explicit retrieval from the vector store, assembles the top-k contexts,\n",
    "injects them into a prompt, and then calls the LLM. This shows the retrieve -> prompt -> generate flow used in RAG.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 documents.\n",
      "Context length: 4888 characters\n",
      "Context preview (first 200 chars): Celebrate new life when it come back around\n",
      "The purpose is in the lessons we learnin' now\n",
      "Sacrifice personal gain over everything\n",
      "Just to see the next generation better than ours\n",
      "I wasn't perfect, the\n",
      "\n",
      "--- Invoking LLM ---\n",
      "\n",
      "--- LLM Response (type: str) ---\n",
      "Result length: 314 characters\n",
      "Result: Hope is the belief or longing that a better outcome is possible, especially in the face of hardship. It shows up as a drive to find peace, healing, or paradise despite struggle. In these lyrics, hope is voiced in lines like ‚ÄúI hope you find some peace of mind in this lifetime‚Äù and ‚ÄúI hope you find some paradise.‚Äù\n",
      "\n",
      "--- LLM Response (type: str) ---\n",
      "Result length: 314 characters\n",
      "Result: Hope is the belief or longing that a better outcome is possible, especially in the face of hardship. It shows up as a drive to find peace, healing, or paradise despite struggle. In these lyrics, hope is voiced in lines like ‚ÄúI hope you find some peace of mind in this lifetime‚Äù and ‚ÄúI hope you find some paradise.‚Äù\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "# Retrieve top-k documents for a question, assemble context, then call the LLM\n",
    "query = \"What is hope?\"\n",
    "# retrieve (uses the FAISS vectorstore created earlier)\n",
    "docs = vectorstore.similarity_search(query, k=5)\n",
    "print(f\"Retrieved {len(docs)} documents.\")\n",
    "# assemble a context string (include a separator and any small provenance if available)\n",
    "context = \"\\n\\n---\\n\\n\".join([getattr(d, 'page_content', str(d)) for d in docs])\n",
    "print(f\"Context length: {len(context)} characters\")\n",
    "print(f\"Context preview (first 200 chars): {context[:200]}\")\n",
    "\n",
    "# build a concise prompt that includes the retrieved context and the original question\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question. Cite sources when helpful.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer (max 3 sentences):\"\"\"\n",
    "\n",
    "filled_prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-nano-2025-08-07\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "llm_chain = filled_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"\\n--- Invoking LLM ---\")\n",
    "result = llm_chain.invoke({\"context\": context, \"question\": query})\n",
    "\n",
    "# display the model output\n",
    "print(f\"\\n--- LLM Response ---\")\n",
    "print(f\"Result length: {len(result) if result else 0} characters\")\n",
    "print(f\"Result: {result if result else '(empty string returned)'}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
