{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3421,
     "status": "ok",
     "timestamp": 1706680163403,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "lYso775DvQdr",
    "outputId": "b3e4e13c-5095-4892-b64b-e6136c230202"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your HF API key: \")\n",
    "\n",
    "open_ai_model = \"gpt-5-nano-2025-08-07\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-za_e1Ovs8O"
   },
   "source": [
    "## Self-Consistency Prompting in Language Models\n",
    "\n",
    "Self-consistency prompting was introduced in the March 2022 paper [\"Self-Consistency Improves Chain of Thought Reasoning in Language Models\"](https://arxiv.org/pdf/2203.11171.pdf) by Xuezhi Wang, et. al.\n",
    "\n",
    "# ü§î **What is Self-Consistency Prompting?**\n",
    "\n",
    "- Focuses on exploring different reasoning paths for complex problems.\n",
    "\n",
    "- Aims for reliable answers by checking consistency across various thought processes.\n",
    "\n",
    "### üåü **Differences from Other Prompting Techniques**\n",
    "\n",
    "- Traditional CoT: Generates short sentences mimicking human reasoning for solving tasks.\n",
    "\n",
    "- Self-Consistency: Samples multiple reasoning paths and finds the most consistent answer.\n",
    "\n",
    "- It's unsupervised and works with pre-trained models, unlike methods needing extra training or human annotations.\n",
    "\n",
    "### **üõ†Ô∏è Constructing a Self-Consistency Prompt**\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://pbs.twimg.com/media/Ff73Y-RaEAANaqT.jpg\" alt=\"Image Description\" style=\"width:100%\">\n",
    "  <figcaption>The self-consistency method contains three steps: (1) prompt a language model using chain-of-thought (CoT) prompting; (2) replace the ‚Äúgreedy decode‚Äù in CoT prompting by sampling from the language model‚Äôs decoder to generate a diverse set of reasoning paths; and (3) marginalize out the reasoning paths and aggregate by choosing the most consistent answer in the final answer set.</figcaption>\n",
    "  <a href=\"https://twitter.com/denny_zhou/status/1584978661668966401\">Image Source</a>\n",
    "</figure>\n",
    "\n",
    "\n",
    "1. **Start with CoT**: Use chain-of-thought prompting as the base.\n",
    "\n",
    "2. **Sample Diverse Paths**: Use \"sample-and-marginalize\" decoding to get various reasoning paths.\n",
    "\n",
    "3. **Marginalize for Consistency**: Find the most consistent answer from these different paths.\n",
    "\n",
    "### üîç **Examples in Action**\n",
    "\n",
    "- **Chain-of-Thought**: For the question \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\", instead of directly answering \"5\", the model might respond with the reasoning: \"There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\"\n",
    "\n",
    "- **Self-Consistency**: For a question about how much Janet makes from selling eggs, the model might generate multiple reasoning paths like:\n",
    "  1. \"She has 16 - 3 - 4 = 9 eggs left. So she makes $2 * 9 = $18 per day.\"\n",
    "\n",
    "  2. \"This means she sells the remainder for $2 * (16 - 4 - 3) = $26 per day.\"\n",
    "  \n",
    "  3. \"She eats 3 for breakfast, so she has 16 - 3 = 13 left. Then she bakes muffins, so she has 13 - 4 = 9 eggs left. So she has 9 eggs * $2 = $18.\"\n",
    "\n",
    "  The model then aggregates these paths to determine the most consistent answer, which in this case is \"$18 per day.\"\n",
    "\n",
    "\n",
    "üöÄ **Impact of Self-Consistency Prompting**\n",
    "- Enhances model reasoning by considering multiple paths.\n",
    "- Shown to boost performance in arithmetic and commonsense reasoning tasks.\n",
    "\n",
    "# Begin by setting up CoT prompts\n",
    "\n",
    "We're following the same pattern from the Chain of Thought lesson.\n",
    "\n",
    "1. Downloading CoT prompt datasets from HuggingFace\n",
    "\n",
    "2. Downloading embeddings model from HuggingFace\n",
    "\n",
    "3. Creating prompt template for CoT\n",
    "\n",
    "4. Creating an example selector\n",
    "\n",
    "5. Construct the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQTIOjPeVTCB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mo/IADT_local/25_26/AI/Prompt_Engineering_LangChain/Prompt_Engineering_LangChain/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset using streaming mode to avoid the file structure issue\n",
    "# Then materialize a subset of it\n",
    "dataset_stream = load_dataset(\n",
    "    \"kaist-ai/CoT-Collection\", \n",
    "    streaming=True,\n",
    "    token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    ")\n",
    "\n",
    "# Take the first 10,000 examples from the training split\n",
    "dataset = dataset_stream['train'].take(10_000) #type: ignore\n",
    "\n",
    "# Convert to a list to materialize the data\n",
    "dataset = list(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the list of examples to a pandas DataFrame\n",
    "dataset_df = pd.DataFrame(dataset) #type: ignore\n",
    "\n",
    "# Remove the 'task' and 'type' columns if they exist\n",
    "columns_to_remove = [col for col in ['task', 'type'] if col in dataset_df.columns]\n",
    "if columns_to_remove:\n",
    "    dataset_df = dataset_df.drop(columns=columns_to_remove)\n",
    "\n",
    "dataset_df.head()\n",
    "\n",
    "selected_examples = dataset_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fZW_bkcHWsIj"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "\n",
    "\n",
    "# from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "# model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "\n",
    "# encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "# embeddings = HuggingFaceBgeEmbeddings(\n",
    "#     model_name=model_name,\n",
    "#     model_kwargs={'device': 'cuda'},\n",
    "#     encode_kwargs=encode_kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OU_F3tjCWtrd"
   },
   "outputs": [],
   "source": [
    "from langchain_core.example_selectors import MaxMarginalRelevanceExampleSelector\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "prefix = \"Consider the following as examples of how to reason:\"\n",
    "\n",
    "examples_template = \"\"\"Query: {source}\n",
    "\n",
    "Rationale: {rationale}\n",
    "\n",
    "Response: {target}\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"Using a similar reasoning approach, answer the users question which is delimited by triple backticks.\n",
    "\n",
    "User question: ```{input}```\n",
    "\n",
    "Take a deep breath, break down the user's query step-by-step, and provide a clear chain of thought in your response.\"\n",
    "\"\"\"\n",
    "\n",
    "examples_prompt = PromptTemplate(\n",
    "    input_variables=[\"source\", \"rationale\", \"target\"],\n",
    "    template=examples_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OrBTG72YXnF5"
   },
   "outputs": [],
   "source": [
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "    selected_examples,\n",
    "    embeddings,\n",
    "    FAISS,\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "mmr_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=examples_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Uk_MsWVrYyAn"
   },
   "outputs": [],
   "source": [
    "query = \"\"\"There were nine computers in the server room. Five more computers were installed each day, from\n",
    "monday to thursday. How many computers are now in the server room?\n",
    "\"\"\"\n",
    "prompt = mmr_prompt.format(input=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1706680539901,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "Tfa1Joqbi0ig",
    "outputId": "129c5a91-e2bc-4107-a15b-b05539eb243d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider the following as examples of how to reason:\n",
      "\n",
      "Query: In this task, you need to answer the given multiple-choice question on the gain. Gain is the value by which to multiply the input. Classify your answers into 'a', 'b', 'c', 'd', and 'e'.\n",
      "\n",
      "Problem: running at the same rate , 8 identical machines can produce 560 paperclips a minute . at this rate , how many paperclips could 18 machines produce in 6 minutes ? \n",
      "Options: a ) 1344 , b ) 3360 , c ) 7560 , d ) 50400 , e ) 67200\n",
      "\n",
      "Rationale: The machines produce 560 paperclips a minute when there are 8 of them. Since each machine produces the same amount, we can say that each one produces 560 / 8 = 70 paperclips a minute.\\nThere are 18 machines producing at this rate, so they produce in total 18 * 70 = 1260 paperclips per minute. In 6 minutes, they will have produced 6 * 1260 = 7560 paperclips.\n",
      "\n",
      "Response: c\n",
      "\n",
      "\n",
      "Query: In this task, you need to count the number of words in a sentence that contain the given letter\n",
      "\n",
      "Sentence: 'two computer monitors are sitting side by side on the desk'. How many words contain the letter 'h' in the sentence.\n",
      "\n",
      "Rationale: Given the sentence 'two computer monitors are sitting side by side on the desk', let's try one word-by-word.\\n1. 'two' : 0 -> (total) 0\\n2. 'computer' : 1 -> (total) 1\\n3. 'monitors' : 0 -> (total) 1\\n4. 'are' : 0 -> (total) 1\\n5. 'sitting' : 0 -> (total) 1 \\n6. 'side': 0->\n",
      "\n",
      "Response: 1\n",
      "\n",
      "\n",
      "Query: Sara's sister has 4 small notebooks in her closet. Last summer she ordered 6 more notebooks and then lost 2. How many notebooks does Sara's sister have now?\n",
      "\n",
      "Rationale: Sara's sister has 4 + 6 = 10 notebooks. After she loses 2, she will have: 10 - 2 = 8 notebooks.\n",
      "\n",
      "Response: 8\n",
      "\n",
      "\n",
      "Query: You are given a math word problem and you are supposed to apply a single mathematical operators like addition, subtraction, multiplication or division on the numbers embedded in the text to answer the following question and then only report final the numerical answer.\n",
      "\n",
      "Donald has 4 oranges . He finds another 5 . How many oranges does Donald have in all ?\n",
      "\n",
      "Rationale: Donald has 4 oranges. He finds another 5 oranges. So, he now has 9 oranges in all.\n",
      "\n",
      "Response: 9\n",
      "\n",
      "\n",
      "Query: Given a sentence, fill out the missing word with a 'no' or a number (between zero and ten). You should write the numbers with english alphabet, like: four instead of 4.\n",
      "\n",
      "Cricket usually lays about five eggs, one every other day for ____ days.\n",
      "\n",
      "Rationale: The sentence states that a cricket lays an egg every other day, which means it lays five eggs in ten days.\n",
      "\n",
      "Response: ten\n",
      "\n",
      "\n",
      "Using a similar reasoning approach, answer the users question which is delimited by triple backticks.\n",
      "\n",
      "User question: ```There were nine computers in the server room. Five more computers were installed each day, from\n",
      "monday to thursday. How many computers are now in the server room?\n",
      "```\n",
      "\n",
      "Take a deep breath, break down the user's query step-by-step, and provide a clear chain of thought in your response.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gpf4qBTGayyu"
   },
   "source": [
    "# Self-Consistency Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AYl6DmB2ay3w"
   },
   "outputs": [],
   "source": [
    "sc_template = \"\"\"Based on the responses (delimited by < >) to the following query, \\\n",
    "(delimited by triple backticks) return the response that occurs most frequently.\n",
    "\n",
    "Query: ```{query}```\n",
    "\n",
    "Responses: <{responses}>\n",
    "\"\"\"\n",
    "\n",
    "sc_prompt = PromptTemplate.from_template(sc_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HiZ4ruMXtVx"
   },
   "source": [
    "### Generating multiple responses\n",
    "\n",
    "\n",
    "Use the `n` parameter to generate alternative responses. Increase `n` to explore different variations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VE0qVJOKaXhE"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "# we'll use the default model here, gpt-3.5-turbo-instruct\n",
    "llm = OpenAI(n=5)\n",
    "\n",
    "generations = llm.generate([prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1706680589103,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "etiUjc8GfA8j",
    "outputId": "f79c3567-f2bf-48b7-d6b2-3064af2ba57b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Generation(text='\\nRationale: The statement indicates that there were 9 computers in the server room initially. Then, 5 more computers were installed each day from Monday to Thursday. This means that on Monday, there were 9 + 5 = 14 computers, on Tuesday there were 14 + 5 = 19 computers, on Wednesday there were 19 + 5 = 24 computers, and on Thursday there were 24 + 5 = 29 computers. Therefore, there are now 29 computers in the server room.\\n\\nResponse: 29', generation_info={'finish_reason': 'stop', 'logprobs': None}),\n",
       "  Generation(text='\\nRationale: The sentence states that there were nine computers in the server room. Each day, five more computers were installed from Monday to Thursday. So, in total, there were 9 + 5 + 5 + 5 + 5 = 29 computers installed in those four days. Therefore, there are now 9 + 29 = 38 computers in the server room.\\n\\nResponse: 38', generation_info={'finish_reason': 'stop', 'logprobs': None}),\n",
       "  Generation(text='\\nProblem: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\\n\\nOptions: a) 13, b) 19, c) 21, d) 28, e) 35\\n\\nRationale: The problem states that there were nine computers in the server room, and five more were installed each day from Monday to Thursday. This means that on Monday, there were 9 computers, on Tuesday there were 9 + 5 = 14 computers, on Wednesday there were 14 + 5 = 19 computers, and on Thursday there were 19 + 5 = 24 computers. Therefore, there are now 24 computers in the server room.\\n\\nResponse: d', generation_info={'finish_reason': 'stop', 'logprobs': None}),\n",
       "  Generation(text='\\nRationale: In the server room, there were nine computers initially. From Monday to Thursday, five more computers were installed each day. This means that by Thursday, there were five * 4 = 20 additional computers installed. Therefore, the total number of computers in the server room is 9 + 20 = 29.\\n\\nResponse: 29', generation_info={'finish_reason': 'stop', 'logprobs': None}),\n",
       "  Generation(text='\\nRationale: We start with 9 computers in the server room. Then, 5 more computers are installed each day from Monday to Thursday. So, in total, we have 9 + 5 + 5 + 5 + 5 = 29 computers in the server room. Therefore, there are now 29 computers in the server room.', generation_info={'finish_reason': 'stop', 'logprobs': None})]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generations.generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "pSww4BDffMLe"
   },
   "outputs": [],
   "source": [
    "responses = []\n",
    "for item in generations.generations[0]:\n",
    "    response_index = item.text.find(\"Response: \")\n",
    "    if response_index != -1:\n",
    "        response = item.text[response_index + len(\"Response: \"):].strip()\n",
    "        responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 143,
     "status": "ok",
     "timestamp": 1706680599295,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "DhXPkzdggX0P",
    "outputId": "bc394caa-6bad-4107-c1e5-b15c784cc916"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['29', '38', 'd', '29']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1706680599852,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "6mNnbPddhGK6",
    "outputId": "0c6cc2ed-921c-45f1-d606-966681fc9ae7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the responses (delimited by < >) to the following query, (delimited by triple backticks) return the response that occurs most frequently.\n",
      "\n",
      "Query: ```There were nine computers in the server room. Five more computers were installed each day, from\n",
      "monday to thursday. How many computers are now in the server room?\n",
      "```\n",
      "\n",
      "Responses: <['29', '38', 'd', '29']>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI()\n",
    "\n",
    "final_prompt = sc_prompt.format(query=query, responses=str(responses))\n",
    "\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 790,
     "status": "ok",
     "timestamp": 1706680603585,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "gjT78-f5jOk8",
    "outputId": "fc43e9ea-8269-4253-c4ed-9e2304257509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(final_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s347K3K_F08g"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN8qnWLWj4rzGsQN8qWXs59",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
