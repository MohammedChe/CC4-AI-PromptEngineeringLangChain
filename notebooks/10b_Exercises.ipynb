{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b47af4",
   "metadata": {},
   "source": [
    "# LangChain Practice Exercises\n",
    "\n",
    "Complete each exercise in order. You can refer back to notebooks `01` through `10` for examples of prompt templates, chat models, chains, retrieval workflows, and more. Use your own API keys via environment variables (see earlier notebooks) and feel free to add extra cells beneath each exercise while you work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d5861e",
   "metadata": {},
   "source": [
    "## Exercise 1 – Prompt Template Warm‑Up\n",
    "Using the ideas from `01_Prompting_with_LangChain.ipynb`, create a `PromptTemplate` that turns a topic into a short public‑service announcement. Format the final string and print it so you can inspect the result before sending it to any model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993aa868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build and format your PromptTemplate here\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# example placeholder variables\n",
    "topic = \"community digital safety\"\n",
    "\n",
    "raise NotImplementedError(\"Create a prompt template and format it with your variables.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1856d755",
   "metadata": {},
   "source": [
    "## Exercise 2 – LCEL Chain with ChatOpenAI\n",
    "Following the workflows in `02_Local_API_and_OpenAI_examples.ipynb` and `03_Model_I_O.ipynb`, build an LCEL pipeline that:\n",
    "1. Accepts a user topic.\n",
    "2. Uses a `ChatPromptTemplate` plus a system instruction for tone.\n",
    "3. Pipes the prompt to `ChatOpenAI` and parses the response with `StrOutputParser`.\n",
    "Print the generated text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the LCEL pipeline (prompt | model | parser)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "raise NotImplementedError(\"Compose the runnable chain and invoke it with a topic.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0194beb",
   "metadata": {},
   "source": [
    "## Exercise 3 – Routing with `RunnableBranch`\n",
    "Using ideas from `05_Chains.ipynb` and `06_Agents.ipynb`, build a small router that dispatches a question to one of two prompts:\n",
    "- If the topic contains the word \"history\", use a prompt that answers as a historian.\n",
    "- Otherwise, use a general helper prompt.\n",
    "Use `RunnableBranch` to select the prompt, feed it through `ChatOpenAI`, and print the model response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f943612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mo/IADT_local/25_26/AI/Prompt_Engineering_LangChain/Prompt_Engineering_LangChain/.venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "/Users/mo/IADT_local/25_26/AI/Prompt_Engineering_LangChain/Prompt_Engineering_LangChain/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Create the branch, attach the model, and test with two questions.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutput_parsers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreate the branch, attach the model, and test with two questions.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNotImplementedError\u001b[39m: Create the branch, attach the model, and test with two questions."
     ]
    }
   ],
   "source": [
    "# TODO: Build the RunnableBranch router and invoke it\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "raise NotImplementedError(\"Create the branch, attach the model, and test with two questions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8613fa93",
   "metadata": {},
   "source": [
    "## Exercise 4 – Retrieval on `kdot.txt`\n",
    "Revisit `04_Retrieval.ipynb` (and the later notebooks on memory/prompt management) to build a minimal RAG flow backed by `../data/kdot.txt`:\n",
    "1. Load the lyrics with `TextLoader` (UTF‑8 encoding).\n",
    "2. Split them with `RecursiveCharacterTextSplitter`.\n",
    "3. Create embeddings with `OpenAIEmbeddings` and store them in a FAISS vector store.\n",
    "4. Ask a question about a recurring theme in Kendrick Lamar’s writing and print the answer.\n",
    "Feel free to reuse helper code from prior notebooks, but keep everything self‑contained in this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f601b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the retrieval flow over ../data/kdot.txt\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "raise NotImplementedError(\"Load the lyrics, build the vector store, and answer a question.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6303cdd9",
   "metadata": {},
   "source": [
    "/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
