{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acc60909",
   "metadata": {},
   "source": [
    "# A Guide to Prompt Templates in LangChain\n",
    "\n",
    "References: [this blog](https://mirascope.com/blog/langchain-prompt-template)\n",
    "\n",
    "A LangChain prompt template helps keep your prompts consistent and is a reusable way to build prompts for language models. \n",
    "\n",
    "\n",
    "A prompt template usually consists of two things:\n",
    "\n",
    "1. A text prompt, which is just a chunk of natural language. It can be plain text, or it can have placeholders like `{variable}` that get filled in with real values when you use it.\n",
    "\n",
    "2. Optional formatting rules, so you can control how the final prompt looks, like whether text should be bold, in all caps, or styled a certain way.\n",
    "\n",
    "Once you fill in the variables, the template turns into a finished prompt that gets sent to the LLM to generate a response.\n",
    "\n",
    "## 3 Types of LangChain Prompt Templates\n",
    "\n",
    "LangChain offers different template classes:\n",
    "\n",
    "- `PromptTemplate` for creating basic string prompts.\n",
    "- `ChatPromptTemplate` for chat-based prompts with multiple messages.\n",
    "- `MessagesPlaceholder` for injecting a dynamic list of messages (such as a conversation history)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a72464",
   "metadata": {},
   "source": [
    "### `PromptTemplate`: Simple String-Based Prompts\n",
    "This generates prompts by filling in blanks in a string.\n",
    "\n",
    "You define a prompt with placeholders using standard Python format syntax. At runtime, these get resolved to generate the final prompt string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde6a0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Share an interesting fact about octopus.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Share an interesting fact about {animal}.\")  # infers 'animal' as input variable\n",
    "\n",
    "# Format the template with a specific animal\n",
    "filled_prompt = example_prompt.format(animal=\"octopus\")\n",
    "print(filled_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdf09a3",
   "metadata": {},
   "source": [
    "You can have multiple placeholders like `{animal}` and `{topic}` as needed, or none at all if the prompt is always the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abef1d3",
   "metadata": {},
   "source": [
    "### `ChatPromptTemplate`: Chat-Style Prompt with Roles\n",
    "\n",
    "Chat-based models (like GPT-4, Claude, or Gemini) don’t just take in a plain string. They expect a sequence of messages where each message has a role (like \"system\", \"user\", or \"assistant\") and some content.\n",
    "\n",
    "LangChain's `ChatPromptTemplate` helps you build these kinds of prompts cleanly and dynamically. Think of it as a way to define a chat scenario with placeholders, then fill in the values when needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a490f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a patient tutor who explains things clearly.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Can you explain gravity like I'm five?\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a patient tutor who explains things clearly.\"),\n",
    "    (\"human\", \"Can you explain {concept} like I'm five?\")\n",
    "])\n",
    "\n",
    "# Fill in the template with a specific concept\n",
    "formatted_messages = chat_prompt.format_messages(concept=\"gravity\")\n",
    "\n",
    "print(formatted_messages)\n",
    "\n",
    "# [\n",
    "#     SystemMessage(content=\"You are a patient tutor who explains things clearly.\", role=\"system\"),\n",
    "#     HumanMessage(content=\"Can you explain gravity like I'm five?\", role=\"user\")\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce32526c",
   "metadata": {},
   "source": [
    "Each item in that list is a structured message that includes both the role and the content, exactly how chat-based LLMs expect it. This is handy because you don’t need to manually construct message objects — the template handles it for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfbc2b3",
   "metadata": {},
   "source": [
    "### `MessagesPlaceholder`: Inserting Dynamic Chat History into a Prompt\n",
    "\n",
    "\n",
    "When you’re working with chat-based models, you often want to include conversation history (or some sequence of messages). MessagesPlaceholder acts as a stand-in for a dynamic list of messages you’ll provide at runtime.\n",
    "\n",
    "Imagine we’re building a career coach bot that remembers previous questions and answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d129aa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful career coach.\"),\n",
    "    MessagesPlaceholder(\"conversation\"),  # Dynamic history insertion\n",
    "    (\"human\", \"{current_question}\")\n",
    "])\n",
    "\n",
    "# Define history using proper message objects\n",
    "conversation_history = [\n",
    "    HumanMessage(content=\"How do I prepare for a job interview?\"),\n",
    "    AIMessage(content=\"Start by researching the company and practicing common questions.\")\n",
    "]\n",
    "\n",
    "formatted_messages = chat_prompt.format_messages(\n",
    "    conversation=conversation_history,\n",
    "    current_question=\"Should I send a thank-you email afterward?\"\n",
    ")\n",
    "\n",
    "print(formatted_messages)\n",
    "\n",
    "# [\n",
    "#     SystemMessage(content=\"You are a helpful career coach.\", role=\"system\"),\n",
    "#     HumanMessage(content=\"How do I prepare for a job interview?\", role=\"user\"),\n",
    "#     AIMessage(content=\"Start by researching the company and practicing common questions.\", role=\"assistant\"),\n",
    "#     HumanMessage(content=\"Should I send a thank-you email afterward?\", role=\"user\")\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e28262",
   "metadata": {},
   "source": [
    "This setup allows you to manage dynamic dialogue context without manual overhead. When we used MessagesPlaceholder(\"conversation\"), it was replaced with the full history of back-and-forth messages between the user and assistant.\n",
    "\n",
    "The messages were injected exactly in order, so you didn’t have to manually merge or format them, since LangChain took care of that behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3f7828",
   "metadata": {},
   "source": [
    "All of the above can be achieved without LangChain but you will need to create your own system. The LangChain prompts library gives you functions that help with prompt engineering and prompt management. You can also store prompt templates online in the LangSmith Platform [see here](https://eu.smith.langchain.com/prompts?organizationId=cc91df9b-ee35-4327-97aa-aa60f6e768cf&q=)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
